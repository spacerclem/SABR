{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# ---------------------\n",
    "# Load and preprocess data\n",
    "# ---------------------\n",
    "data = pd.read_csv('data.csv')  # Replace with your actual data file\n",
    "assert data.shape[1] == 21, \"Data must have 21 features.\"\n",
    "\n",
    "X = data.values.astype(np.float32)\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_data = torch.tensor(X_train, dtype=torch.float32)\n",
    "val_data = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_data)\n",
    "val_dataset = TensorDataset(val_data)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 21\n",
    "latent_dim = 4     # dimension of the latent space\n",
    "hidden_dim = 64    # hidden dimension for encoder/decoder\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "patience = 10\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ---------------------\n",
    "# Define the VAE model with Gaussian decoder\n",
    "# ---------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mu_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder: outputs mean and logvar for each input dimension\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            # Output: mean and logvar for each dimension => 2 * input_dim\n",
    "            nn.Linear(hidden_dim, 2 * input_dim)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        params = self.decoder(z)\n",
    "        # split into mean and logvar\n",
    "        mean = params[:, :input_dim]\n",
    "        logvar = params[:, input_dim:]\n",
    "        return mean, logvar\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        mean, logvar_out = self.decode(z)\n",
    "        return mean, logvar_out, mu, logvar\n",
    "\n",
    "# ---------------------\n",
    "# Loss function (Gaussian decoder)\n",
    "# ---------------------\n",
    "def gaussian_nll(x, mean, logvar):\n",
    "    # Negative log likelihood of x under Gaussian(mean, var=exp(logvar))\n",
    "    # For each feature dimension:\n",
    "    # NLL = 0.5*(log(2*pi) + logvar + (x-mean)^2/exp(logvar))\n",
    "    # sum over features\n",
    "    D = x.size(1)\n",
    "    const = D * 0.5 * np.log(2 * np.pi)\n",
    "    logvar_sum = 0.5 * torch.sum(logvar, dim=1)\n",
    "    inv_var = torch.exp(-logvar)\n",
    "    sq_error = (x - mean)**2\n",
    "    mse_term = 0.5 * torch.sum(sq_error * inv_var, dim=1)\n",
    "    nll = const + logvar_sum + mse_term\n",
    "    return torch.sum(nll)  # sum over batch\n",
    "\n",
    "def loss_function(mean, logvar_out, x, mu, logvar_enc):\n",
    "    # Gaussian decoder NLL\n",
    "    recon_loss = gaussian_nll(x, mean, logvar_out)\n",
    "    # KL Divergence\n",
    "    kld = -0.5 * torch.sum(1 + logvar_enc - mu.pow(2) - logvar_enc.exp())\n",
    "    return recon_loss + kld\n",
    "\n",
    "# ---------------------\n",
    "# Training setup\n",
    "# ---------------------\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = np.inf\n",
    "waiting = 0\n",
    "ckpt_path = 'vae_gaussian_decoder_checkpoint.pt'\n",
    "\n",
    "# ---------------------\n",
    "# Training loop\n",
    "# ---------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mean, logvar_out, mu, logvar_enc = model(x)\n",
    "        loss = loss_function(mean, logvar_out, x, mu, logvar_enc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch[0].to(device)\n",
    "            mean, logvar_out, mu, logvar_enc = model(x)\n",
    "            loss = loss_function(mean, logvar_out, x, mu, logvar_enc)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        waiting = 0\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "    else:\n",
    "        waiting += 1\n",
    "        if waiting > patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.eval()\n",
    "\n",
    "# ---------------------\n",
    "# Generate samples from VAE\n",
    "# ---------------------\n",
    "def sample_from_vae(model, num_samples=1000):\n",
    "    z = torch.randn(num_samples, latent_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        mean, logvar_out = model.decode(z)\n",
    "    # Sample from N(mean, exp(logvar_out))\n",
    "    std = torch.exp(0.5 * logvar_out)\n",
    "    eps = torch.randn_like(std)\n",
    "    samples = mean + eps * std\n",
    "    return samples.cpu().numpy()\n",
    "\n",
    "gen_samples = sample_from_vae(model, num_samples=len(data))\n",
    "gen_samples_orig = scaler.inverse_transform(gen_samples)\n",
    "\n",
    "real_data = data.values\n",
    "\n",
    "# ---------------------\n",
    "# Visualization\n",
    "# ---------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot expiry distribution\n",
    "sns.kdeplot(real_data[:,0], label='Real', ax=axes[0], shade=True)\n",
    "sns.kdeplot(gen_samples_orig[:,0], label='Generated', ax=axes[0], shade=True)\n",
    "axes[0].set_title('Expiry Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot forward distribution\n",
    "sns.kdeplot(real_data[:,2], label='Real', ax=axes[1], shade=True)\n",
    "sns.kdeplot(gen_samples_orig[:,2], label='Generated', ax=axes[1], shade=True)\n",
    "axes[1].set_title('Forward Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot one strike and one vol dimension\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "strike_index = 3  # strikes start at index 3\n",
    "vol_index = 3 + 9 # after strikes come the vols\n",
    "\n",
    "sns.kdeplot(real_data[:, strike_index], label='Real Strike', ax=axes[0], shade=True)\n",
    "sns.kdeplot(gen_samples_orig[:, strike_index], label='Generated Strike', ax=axes[0], shade=True)\n",
    "axes[0].set_title(f'Strike {strike_index - 2} Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "sns.kdeplot(real_data[:, vol_index], label='Real Vol', ax=axes[1], shade=True)\n",
    "sns.kdeplot(gen_samples_orig[:, vol_index], label='Generated Vol', ax=axes[1], shade=True)\n",
    "axes[1].set_title(f'Vol {vol_index - 11} Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
