{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "import itertools\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bachelier_call_price(forward, strike, vol, time_to_expiry):\n",
    "    \"\"\"\n",
    "    forward, strike, vol, time_to_expiry: Tensors of the same shape\n",
    "    Bachelier formula for European call options:\n",
    "      C = sigma * sqrt(T) * phi(d) + (F - K)*Phi(d),\n",
    "      d = (F - K)/(sigma * sqrt(T))\n",
    "    \"\"\"\n",
    "    # Avoid zero or negative vol or time to avoid Inf\n",
    "    eps = 1e-8\n",
    "    vol = torch.clamp(vol, min=eps)\n",
    "    time_to_expiry = torch.clamp(time_to_expiry, min=eps)\n",
    "    \n",
    "    d = (forward - strike) / (vol * torch.sqrt(time_to_expiry))\n",
    "    \n",
    "    normal = Normal(0.0, 1.0)\n",
    "    pdf_d = torch.exp(normal.log_prob(d))  # phi(d)\n",
    "    cdf_d = normal.cdf(d)                  # Phi(d)\n",
    "    \n",
    "    call_price = vol * torch.sqrt(time_to_expiry) * pdf_d + (forward - strike) * cdf_d\n",
    "    return call_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------\n",
    "# Define the VAE model\n",
    "# ---------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mu_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inverse_transform_gpu(x_standard: torch.Tensor, scale_torch, mean_torch) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Inverse StandardScaler transform on GPU:\n",
    "    x_original = x_standard * scale + mean\n",
    "    \"\"\"\n",
    "    return x_standard * scale_torch + mean_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butterfly_arbitrage_penalty(x_std, recon_std, scale_torch, mean_torch):\n",
    "    \"\"\"\n",
    "    x_std, recon_std: [batch_size, 21] in standardized scale.\n",
    "    We'll compute butterfly penalty on the reconstructed data \n",
    "    (i.e., ensure the model doesn't produce negative convexities).\n",
    "    \n",
    "    Steps:\n",
    "      1. Inverse-transform from standardized -> original scale on GPU.\n",
    "      2. Extract relevant columns: \n",
    "         expiry (col 0), forward (col 2), strikes (3..11), vols (12..20)\n",
    "      3. Compute Bachelier call prices at each strike.\n",
    "      4. Discrete second derivative of call prices w.r.t. strike.\n",
    "      5. Accumulate negative parts -> penalty.\n",
    "    \"\"\"\n",
    "    # 1) Inverse transform on GPU\n",
    "    recon_orig = inverse_transform_gpu(recon_std, scale_torch, mean_torch)\n",
    "    \n",
    "    # 2) Extract columns from the *generated* data\n",
    "    expiry_gen  = recon_orig[:, 0]        # time to expiry\n",
    "    forward_gen = recon_orig[:, 2]\n",
    "    strikes_gen = recon_orig[:, 3:12]\n",
    "    vols_gen    = recon_orig[:, 12:21]\n",
    "    \n",
    "    # We'll apply penalty only on the reconstructed surfaces \n",
    "    # to encourage no butterfly arbitrage in the generated data.\n",
    "    \n",
    "    penalty_sum = 0.0\n",
    "    batch_size = x_std.size(0)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        T     = expiry_gen[i]        # scalar\n",
    "        F     = forward_gen[i]       # scalar\n",
    "        Ks    = strikes_gen[i, :]    # shape [9]\n",
    "        sigs  = vols_gen[i, :]       # shape [9]\n",
    "        \n",
    "        # Repeat T, F to match shape [9] if needed\n",
    "        T_vec = T.unsqueeze(0).expand_as(Ks)  # shape [9]\n",
    "        F_vec = F.unsqueeze(0).expand_as(Ks)  # shape [9]\n",
    "        \n",
    "        # Bachelier call prices for 9 strikes\n",
    "        prices = bachelier_call_price(F_vec, Ks, sigs, T_vec / 12.0)  # shape [9]\n",
    "        \n",
    "        # Discrete approximation of second derivative\n",
    "        # slopes_j = (price_{j+1} - price_j) / (K_{j+1} - K_j)\n",
    "        # second_deriv_j = slopes_{j+1} - slopes_j\n",
    "        # sum negative parts\n",
    "        slopes = []\n",
    "        for j in range(8):\n",
    "            dx = Ks[j+1] - Ks[j]\n",
    "            if abs(dx) < 1e-12:  \n",
    "                continue\n",
    "            slope_j = (prices[j+1] - prices[j]) / dx\n",
    "            slopes.append(slope_j)\n",
    "        \n",
    "        penalty_i = 0.0\n",
    "        for j in range(len(slopes) - 1):\n",
    "            second_deriv = slopes[j+1] - slopes[j]\n",
    "            # Penalize negative second derivative\n",
    "            if second_deriv < 0:\n",
    "                penalty_i += -second_deriv\n",
    "        \n",
    "        penalty_sum += penalty_i\n",
    "    \n",
    "    return penalty_sum / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------\n",
    "# Loss function\n",
    "# ---------------------\n",
    "def loss_function(recon, x, mu, logvar):\n",
    "    # (a) Standard VAE: MSE + KL\n",
    "    recon_loss = nn.MSELoss(reduction='sum')(recon, x)\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    base_vae_loss = recon_loss + kld\n",
    "    return base_vae_loss\n",
    "\n",
    "def total_loss_with_butterfly(recon, x, mu, logvar, lambda_butterfly, scale_torch, mean_torch):\n",
    "    # Standard VAE loss\n",
    "    vae_loss = loss_function(recon, x, mu, logvar)\n",
    "    # Butterfly penalty\n",
    "    if lambda_butterfly > 0.:\n",
    "        bfly_pen = butterfly_arbitrage_penalty(x, recon, scale_torch, mean_torch)\n",
    "    else:\n",
    "        bfly_pen = 0.\n",
    "    \n",
    "    # Weighted sum\n",
    "    return vae_loss + lambda_butterfly * bfly_pen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_vae(model, latent_dim, device, num_samples=1000):\n",
    "    z = torch.randn(num_samples, latent_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        samples = model.decode(z).cpu().numpy()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_butterfly_arbitrage(strikes, prices):\n",
    "    \"\"\"\n",
    "    Check that the call price curve is convex in a discrete sense.\n",
    "    We verify that the slope between consecutive points is non-decreasing:\n",
    "      slope_i   = (P[i+1] - P[i]) / (K[i+1] - K[i])\n",
    "      slope_i+1 = (P[i+2] - P[i+1]) / (K[i+2] - K[i+1])\n",
    "    We need slope_i <= slope_i+1 for i=0..(n_strikes-3).\n",
    "    \"\"\"\n",
    "    n = len(strikes)\n",
    "    if n < 3:\n",
    "        return True  # trivial if fewer than 3 points\n",
    "\n",
    "    slopes = []\n",
    "    for i in range(n - 1):\n",
    "        dk = strikes[i+1] - strikes[i]\n",
    "        dp = prices[i+1] - prices[i]\n",
    "        if dk <= 0:\n",
    "            return False  # strikes must be strictly increasing\n",
    "        if dp > 0:\n",
    "            return False\n",
    "        slopes.append(dp / dk)\n",
    "    \n",
    "    # Check that slopes are non-decreasing\n",
    "    for i in range(len(slopes) - 1):\n",
    "        if slopes[i] > slopes[i+1]:\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normal_call_price(forward, strike, vol, expiry_years, notional=1.0):\n",
    "    \"\"\"\n",
    "    Bachelier (normal) model call price, ignoring discounting for simplicity.\n",
    "    \n",
    "    Formula (for zero rates, notional=1):\n",
    "        Call = (F - K)*Phi(d) + sigma * sqrt(T) * phi(d),\n",
    "    where\n",
    "        d = (F - K) / (sigma * sqrt(T))\n",
    "        Phi(.) is standard normal CDF, phi(.) is standard normal PDF.\n",
    "    \n",
    "    If vol = 0 or expiry ~ 0, fallback to intrinsic value: max(F - K, 0).\n",
    "    \"\"\"\n",
    "    if vol < 1e-8 or expiry_years < 1e-8:\n",
    "        return notional * max(forward - strike, 0.0)\n",
    "\n",
    "    sigma_sqrt_t = vol * sqrt(expiry_years)\n",
    "    d = (forward - strike) / sigma_sqrt_t\n",
    "\n",
    "    call_price = notional * ((forward - strike) * norm.cdf(d) \n",
    "                             + sigma_sqrt_t * norm.pdf(d))\n",
    "    return call_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_butterfly_arbitrage_vol(strikes, vols, expiry_years, forward):\n",
    "    prices = [\n",
    "            normal_call_price(forward, K, vol, expiry_years)\n",
    "            for K, vol in zip(strikes, vols)\n",
    "        ]\n",
    "    return check_butterfly_arbitrage(strikes, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_vae(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    data,\n",
    "    scaler,\n",
    "    scale_torch,\n",
    "    mean_torch,\n",
    "    lr=1e-3,\n",
    "    hidden_dim=64,\n",
    "    latent_dim=4,\n",
    "    epochs=20,\n",
    "    device='cuda',\n",
    "    patience=20,\n",
    "    lambda_butterfly = 1000.0,\n",
    "):\n",
    "    \n",
    "    model = VAE(input_dim=21, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=patience, verbose=True)\n",
    "\n",
    "    best_train_loss = np.inf\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    waiting = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            recon, mu, logvar = model(x_batch)\n",
    "            loss = total_loss_with_butterfly(recon, x_batch, mu, logvar, lambda_butterfly, scale_torch, mean_torch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch = batch[0].to(device)\n",
    "                recon, mu, logvar = model(x_batch)\n",
    "                loss = total_loss_with_butterfly(recon, x_batch, mu, logvar, lambda_butterfly, scale_torch, mean_torch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            waiting += 1\n",
    "            if waiting > patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    gen_samples = sample_from_vae(model, latent_dim=latent_dim, device=device, num_samples=len(data))\n",
    "    gen_samples_orig = scaler.inverse_transform(gen_samples)\n",
    "    df_gen = pd.DataFrame(gen_samples_orig, columns=data.columns)\n",
    "\n",
    "    arbitrages_count = 0\n",
    "    for idx in range(len(df_gen)):\n",
    "        gen_strikes = [df_gen[\"K\" + str(x)].values[idx] for x in range(1,10)]\n",
    "        gen_vols = [df_gen[\"vol\" + str(x)].values[idx] for x in range(1,10)]\n",
    "        gen_forward = df_gen[\"forward\"].values[idx]\n",
    "        gen_expiry_years = df_gen[\"expiry_months\"].values[idx] / 12.0\n",
    "\n",
    "        if not check_butterfly_arbitrage_vol(gen_strikes, gen_vols, gen_expiry_years, gen_forward):\n",
    "            arbitrages_count += 1\n",
    "\n",
    "    arbitrages_proportion = arbitrages_count / len(df_gen)\n",
    "\n",
    "    return best_train_loss, best_val_loss, arbitrages_proportion\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    param_grid = {\n",
    "        \"lr\":          [1e-3, 5e-4],       # learning rates to try\n",
    "        \"hidden_dim\":  [64, 128, 256, 512],           # hidden dims to try\n",
    "        \"latent_dim\":  [32, 64, 128, 256],             # latent dims to try\n",
    "        \"batch_size\":  [64, 128, 256],          # batch sizes\n",
    "        \"epochs\":      [50, 100],               # or more\n",
    "        'patience': [10, 20],\n",
    "        'lambda_butterfly': [0., 1000.0, 10000.0],\n",
    "    }\n",
    "\n",
    "\n",
    "    data = pd.read_csv('data.csv')  # Replace with your actual data file\n",
    "    assert data.shape[1] == 22, \"Data must have 22 features.\"\n",
    "\n",
    "    data = data[[x for x in data.columns if x != \"duration\"]]\n",
    "    assert data.shape[1] == 21, \"Data must have 21 features.\"\n",
    "\n",
    "    X = data.values.astype(np.float32)\n",
    "\n",
    "    # Train/validation split\n",
    "    X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    train_data = torch.tensor(X_train, dtype=torch.float32)\n",
    "    val_data = torch.tensor(X_val, dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_data)\n",
    "    val_dataset = TensorDataset(val_data)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    mean_torch = torch.tensor(scaler.mean_, dtype=torch.float32, device=device)\n",
    "    scale_torch = torch.tensor(scaler.scale_, dtype=torch.float32, device=device)\n",
    "\n",
    "    best_train_loss   = float('inf')\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # We'll do a grid search over all param combos\n",
    "    keys = list(param_grid.keys())\n",
    "    for combo in itertools.product(*(param_grid[key] for key in keys)):\n",
    "        params = dict(zip(keys, combo))   # e.g. {\"lr\": 1e-3, \"hidden_dim\": 32, \"latent_dim\": 4, \"batch_size\": 64, \"epochs\": 20}\n",
    "        print(\"Testing hyperparams:\", params)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader   = DataLoader(val_dataset,   batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "        train_loss, val_loss, arbitrages_proportion = train_and_evaluate_vae(\n",
    "            train_loader  = train_loader,\n",
    "            val_loader    = val_loader,\n",
    "            data = data,\n",
    "            scaler = scaler,\n",
    "            scale_torch = scale_torch,\n",
    "            mean_torch = mean_torch,\n",
    "            lr         = params[\"lr\"],\n",
    "            hidden_dim = params[\"hidden_dim\"],\n",
    "            latent_dim = params[\"latent_dim\"],\n",
    "            epochs     = params[\"epochs\"],\n",
    "            device     = device,\n",
    "\n",
    "        )\n",
    "\n",
    "        run_result = {**params, 'train_loss': train_loss, 'val_loss': val_loss, 'arbitrages_proportion': arbitrages_proportion}\n",
    "        results.append(run_result)\n",
    "\n",
    "        print(f\" --> Result: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, arbitrages_proportion={arbitrages_proportion:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss   = val_loss\n",
    "            best_params = params\n",
    "            print(\"New best validation loss:\", best_val_loss, \"with\", best_params)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv('hyperparam_results.csv', index=False)\n",
    "    print(\"\\nAll results saved to 'hyperparam_results.csv'.\")\n",
    "\n",
    "    print(\"\\n=======================================\")\n",
    "    print(\"Best overall val_loss:\", best_val_loss)\n",
    "    print(\"Best hyperparams:\", best_params)\n",
    "    print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/50, Train Loss: 14.0785, Val Loss: 8.7599\n",
      "Epoch 2/50, Train Loss: 8.5411, Val Loss: 8.5002\n",
      "Epoch 3/50, Train Loss: 8.2986, Val Loss: 7.9829\n",
      "Epoch 4/50, Train Loss: 7.9782, Val Loss: 7.5421\n",
      "Epoch 5/50, Train Loss: 7.1581, Val Loss: 6.7969\n",
      "Epoch 6/50, Train Loss: 6.6564, Val Loss: 6.3615\n",
      "Epoch 7/50, Train Loss: 5.9213, Val Loss: 5.5948\n",
      "Epoch 8/50, Train Loss: 5.4378, Val Loss: 5.3251\n",
      "Epoch 9/50, Train Loss: 5.2859, Val Loss: 5.3271\n",
      "Epoch 10/50, Train Loss: 5.2312, Val Loss: 5.2099\n",
      "Epoch 11/50, Train Loss: 5.2648, Val Loss: 5.2279\n",
      "Epoch 12/50, Train Loss: 5.1619, Val Loss: 5.3484\n",
      "Epoch 13/50, Train Loss: 5.3082, Val Loss: 5.6372\n",
      "Epoch 14/50, Train Loss: 5.3370, Val Loss: 5.2617\n",
      "Epoch 15/50, Train Loss: 5.2116, Val Loss: 5.4388\n",
      "Epoch 16/50, Train Loss: 5.1977, Val Loss: 5.1982\n",
      "Epoch 17/50, Train Loss: 5.1867, Val Loss: 5.2611\n",
      "Epoch 18/50, Train Loss: 5.0775, Val Loss: 5.0322\n",
      "Epoch 19/50, Train Loss: 5.1558, Val Loss: 5.1407\n",
      "Epoch 20/50, Train Loss: 5.0646, Val Loss: 5.0595\n",
      "Epoch 21/50, Train Loss: 5.0413, Val Loss: 5.0881\n",
      "Epoch 22/50, Train Loss: 5.1449, Val Loss: 5.5181\n",
      "Epoch 23/50, Train Loss: 5.1494, Val Loss: 5.0889\n",
      "Epoch 24/50, Train Loss: 5.0124, Val Loss: 5.0385\n",
      "Epoch 25/50, Train Loss: 5.0177, Val Loss: 5.0307\n",
      "Epoch 26/50, Train Loss: 5.0120, Val Loss: 5.0151\n",
      "Epoch 27/50, Train Loss: 4.9890, Val Loss: 4.9822\n",
      "Epoch 28/50, Train Loss: 4.9914, Val Loss: 5.0138\n",
      "Epoch 29/50, Train Loss: 4.9913, Val Loss: 5.2283\n",
      "Epoch 30/50, Train Loss: 5.0256, Val Loss: 5.0345\n",
      "Epoch 31/50, Train Loss: 5.0026, Val Loss: 4.9746\n",
      "Epoch 32/50, Train Loss: 4.9925, Val Loss: 4.9482\n",
      "Epoch 33/50, Train Loss: 5.0511, Val Loss: 4.9936\n",
      "Epoch 34/50, Train Loss: 5.0112, Val Loss: 5.0216\n",
      "Epoch 35/50, Train Loss: 4.9703, Val Loss: 4.9965\n",
      "Epoch 36/50, Train Loss: 4.9611, Val Loss: 4.9985\n",
      "Epoch 37/50, Train Loss: 4.9611, Val Loss: 4.9955\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9611, val_loss=4.9482, arbitrages_proportion=0.1151\n",
      "New best validation loss: 4.948174522399903 with {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 0.0}\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 1000.0}\n",
      "Epoch 1/50, Train Loss: 14.7300, Val Loss: 8.5864\n",
      "Epoch 2/50, Train Loss: 8.4151, Val Loss: 8.2945\n",
      "Epoch 3/50, Train Loss: 8.0576, Val Loss: 7.8060\n",
      "Epoch 4/50, Train Loss: 7.4065, Val Loss: 7.0025\n",
      "Epoch 5/50, Train Loss: 6.7554, Val Loss: 6.6112\n",
      "Epoch 6/50, Train Loss: 6.5308, Val Loss: 6.4125\n",
      "Epoch 7/50, Train Loss: 6.1859, Val Loss: 6.0986\n",
      "Epoch 8/50, Train Loss: 5.8857, Val Loss: 5.8030\n",
      "Epoch 9/50, Train Loss: 5.7680, Val Loss: 5.7389\n",
      "Epoch 10/50, Train Loss: 5.4177, Val Loss: 5.2999\n",
      "Epoch 11/50, Train Loss: 5.2028, Val Loss: 5.2058\n",
      "Epoch 12/50, Train Loss: 5.2270, Val Loss: 5.1626\n",
      "Epoch 13/50, Train Loss: 5.1064, Val Loss: 5.3015\n",
      "Epoch 14/50, Train Loss: 5.2252, Val Loss: 5.3627\n",
      "Epoch 15/50, Train Loss: 5.1074, Val Loss: 5.3395\n",
      "Epoch 16/50, Train Loss: 5.2215, Val Loss: 5.1449\n",
      "Epoch 17/50, Train Loss: 5.1407, Val Loss: 5.0442\n",
      "Epoch 18/50, Train Loss: 5.0418, Val Loss: 5.0902\n",
      "Epoch 19/50, Train Loss: 5.0408, Val Loss: 5.0846\n",
      "Epoch 20/50, Train Loss: 5.1324, Val Loss: 5.0536\n",
      "Epoch 21/50, Train Loss: 5.0314, Val Loss: 5.0123\n",
      "Epoch 22/50, Train Loss: 4.9882, Val Loss: 5.0145\n",
      "Epoch 23/50, Train Loss: 5.0875, Val Loss: 4.9956\n",
      "Epoch 24/50, Train Loss: 5.0022, Val Loss: 5.0652\n",
      "Epoch 25/50, Train Loss: 5.0718, Val Loss: 5.0917\n",
      "Epoch 26/50, Train Loss: 4.9591, Val Loss: 5.0368\n",
      "Epoch 27/50, Train Loss: 5.0140, Val Loss: 5.0083\n",
      "Epoch 28/50, Train Loss: 5.0966, Val Loss: 5.0288\n",
      "Epoch 29/50, Train Loss: 4.9848, Val Loss: 4.9785\n",
      "Epoch 30/50, Train Loss: 4.9966, Val Loss: 5.0536\n",
      "Epoch 31/50, Train Loss: 4.9708, Val Loss: 4.9538\n",
      "Epoch 32/50, Train Loss: 4.9478, Val Loss: 4.9792\n",
      "Epoch 33/50, Train Loss: 4.9713, Val Loss: 4.9017\n",
      "Epoch 34/50, Train Loss: 4.9291, Val Loss: 4.9712\n",
      "Epoch 35/50, Train Loss: 4.9930, Val Loss: 4.9829\n",
      "Epoch 36/50, Train Loss: 4.9563, Val Loss: 4.9449\n",
      "Epoch 37/50, Train Loss: 5.0045, Val Loss: 4.9151\n",
      "Epoch 38/50, Train Loss: 4.9760, Val Loss: 4.8773\n",
      "Epoch 39/50, Train Loss: 4.9169, Val Loss: 5.0581\n",
      "Epoch 40/50, Train Loss: 4.9393, Val Loss: 4.9142\n",
      "Epoch 41/50, Train Loss: 4.9625, Val Loss: 5.0357\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9169, val_loss=4.8773, arbitrages_proportion=0.3654\n",
      "New best validation loss: 4.8773353424072265 with {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 1000.0}\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 10000.0}\n",
      "Epoch 1/50, Train Loss: 14.4398, Val Loss: 8.8302\n",
      "Epoch 2/50, Train Loss: 8.4735, Val Loss: 8.2995\n",
      "Epoch 3/50, Train Loss: 7.9199, Val Loss: 7.4588\n",
      "Epoch 4/50, Train Loss: 6.7927, Val Loss: 6.1799\n",
      "Epoch 5/50, Train Loss: 5.7738, Val Loss: 5.4982\n",
      "Epoch 6/50, Train Loss: 5.3692, Val Loss: 5.2988\n",
      "Epoch 7/50, Train Loss: 5.2541, Val Loss: 5.4526\n",
      "Epoch 8/50, Train Loss: 5.2935, Val Loss: 5.5343\n",
      "Epoch 9/50, Train Loss: 5.2138, Val Loss: 5.1812\n",
      "Epoch 10/50, Train Loss: 5.1259, Val Loss: 5.1917\n",
      "Epoch 11/50, Train Loss: 5.2883, Val Loss: 5.3654\n",
      "Epoch 12/50, Train Loss: 5.1379, Val Loss: 5.1028\n",
      "Epoch 13/50, Train Loss: 5.2038, Val Loss: 5.1307\n",
      "Epoch 14/50, Train Loss: 5.1248, Val Loss: 5.1565\n",
      "Epoch 15/50, Train Loss: 5.1265, Val Loss: 5.1216\n",
      "Epoch 16/50, Train Loss: 5.0823, Val Loss: 5.1707\n",
      "Epoch 17/50, Train Loss: 5.0950, Val Loss: 5.0908\n",
      "Epoch 18/50, Train Loss: 5.0742, Val Loss: 5.2282\n",
      "Epoch 19/50, Train Loss: 5.1038, Val Loss: 5.1466\n",
      "Epoch 20/50, Train Loss: 5.0615, Val Loss: 5.0625\n",
      "Epoch 21/50, Train Loss: 5.1872, Val Loss: 5.1729\n",
      "Epoch 22/50, Train Loss: 5.0695, Val Loss: 5.0389\n",
      "Epoch 23/50, Train Loss: 5.1156, Val Loss: 5.0319\n",
      "Epoch 24/50, Train Loss: 5.0214, Val Loss: 5.0208\n",
      "Epoch 25/50, Train Loss: 5.0393, Val Loss: 5.0106\n",
      "Epoch 26/50, Train Loss: 5.0284, Val Loss: 5.0692\n",
      "Epoch 27/50, Train Loss: 5.0475, Val Loss: 5.0585\n",
      "Epoch 28/50, Train Loss: 4.9885, Val Loss: 4.9732\n",
      "Epoch 29/50, Train Loss: 4.9906, Val Loss: 4.9603\n",
      "Epoch 30/50, Train Loss: 4.9907, Val Loss: 5.0869\n",
      "Epoch 31/50, Train Loss: 5.0260, Val Loss: 4.9296\n",
      "Epoch 32/50, Train Loss: 4.9649, Val Loss: 4.9546\n",
      "Epoch 33/50, Train Loss: 5.0016, Val Loss: 5.0631\n",
      "Epoch 34/50, Train Loss: 4.9841, Val Loss: 4.9607\n",
      "Epoch 35/50, Train Loss: 4.9854, Val Loss: 5.1184\n",
      "Epoch 36/50, Train Loss: 5.0126, Val Loss: 4.9894\n",
      "Epoch 37/50, Train Loss: 4.9896, Val Loss: 5.0626\n",
      "Epoch 38/50, Train Loss: 4.9598, Val Loss: 4.9413\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9598, val_loss=4.9296, arbitrages_proportion=0.0983\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 50, 'patience': 20, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/50, Train Loss: 15.2060, Val Loss: 8.7156\n",
      "Epoch 2/50, Train Loss: 8.5077, Val Loss: 8.3798\n",
      "Epoch 3/50, Train Loss: 8.0890, Val Loss: 7.7660\n",
      "Epoch 4/50, Train Loss: 7.3825, Val Loss: 7.0826\n",
      "Epoch 5/50, Train Loss: 6.3839, Val Loss: 5.7272\n",
      "Epoch 6/50, Train Loss: 5.5130, Val Loss: 5.4541\n",
      "Epoch 7/50, Train Loss: 5.6341, Val Loss: 5.5506\n",
      "Epoch 8/50, Train Loss: 5.3739, Val Loss: 5.5394\n",
      "Epoch 9/50, Train Loss: 5.2829, Val Loss: 5.2108\n",
      "Epoch 10/50, Train Loss: 5.1986, Val Loss: 5.2486\n",
      "Epoch 11/50, Train Loss: 5.1808, Val Loss: 5.3980\n",
      "Epoch 12/50, Train Loss: 5.1994, Val Loss: 5.3053\n",
      "Epoch 13/50, Train Loss: 5.1569, Val Loss: 5.2631\n",
      "Epoch 14/50, Train Loss: 5.1332, Val Loss: 5.1611\n",
      "Epoch 15/50, Train Loss: 5.1008, Val Loss: 5.1127\n",
      "Epoch 16/50, Train Loss: 5.0971, Val Loss: 5.1121\n",
      "Epoch 17/50, Train Loss: 5.0692, Val Loss: 5.0989\n",
      "Epoch 18/50, Train Loss: 5.0511, Val Loss: 5.1261\n",
      "Epoch 19/50, Train Loss: 5.0330, Val Loss: 5.1622\n",
      "Epoch 20/50, Train Loss: 5.0884, Val Loss: 5.0376\n",
      "Epoch 21/50, Train Loss: 5.0337, Val Loss: 5.0303\n",
      "Epoch 22/50, Train Loss: 5.0063, Val Loss: 5.0071\n",
      "Epoch 23/50, Train Loss: 5.0516, Val Loss: 5.0628\n",
      "Epoch 24/50, Train Loss: 5.0329, Val Loss: 5.1138\n",
      "Epoch 25/50, Train Loss: 5.0097, Val Loss: 5.0671\n",
      "Epoch 26/50, Train Loss: 5.0237, Val Loss: 4.9994\n",
      "Epoch 27/50, Train Loss: 4.9968, Val Loss: 5.0205\n",
      "Epoch 28/50, Train Loss: 4.9976, Val Loss: 4.9999\n",
      "Epoch 29/50, Train Loss: 4.9535, Val Loss: 4.9716\n",
      "Epoch 30/50, Train Loss: 5.0045, Val Loss: 5.0034\n",
      "Epoch 31/50, Train Loss: 5.0296, Val Loss: 5.0274\n",
      "Epoch 32/50, Train Loss: 5.0778, Val Loss: 5.1114\n",
      "Epoch 33/50, Train Loss: 4.9829, Val Loss: 4.9646\n",
      "Epoch 34/50, Train Loss: 4.9992, Val Loss: 4.9158\n",
      "Epoch 35/50, Train Loss: 4.9822, Val Loss: 5.0191\n",
      "Epoch 36/50, Train Loss: 5.0135, Val Loss: 5.0834\n",
      "Epoch 37/50, Train Loss: 4.9720, Val Loss: 4.9476\n",
      "Epoch 38/50, Train Loss: 5.0176, Val Loss: 5.0233\n",
      "Epoch 39/50, Train Loss: 4.9577, Val Loss: 4.9972\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9535, val_loss=4.9158, arbitrages_proportion=0.0708\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 50, 'patience': 20, 'lambda_butterfly': 1000.0}\n",
      "Epoch 1/50, Train Loss: 14.4800, Val Loss: 8.5741\n",
      "Epoch 2/50, Train Loss: 8.4127, Val Loss: 8.2722\n",
      "Epoch 3/50, Train Loss: 7.8063, Val Loss: 7.6195\n",
      "Epoch 4/50, Train Loss: 6.9750, Val Loss: 6.3722\n",
      "Epoch 5/50, Train Loss: 5.8958, Val Loss: 5.5221\n",
      "Epoch 6/50, Train Loss: 5.4470, Val Loss: 5.3135\n",
      "Epoch 7/50, Train Loss: 5.2502, Val Loss: 5.2721\n",
      "Epoch 8/50, Train Loss: 5.2507, Val Loss: 5.2464\n",
      "Epoch 9/50, Train Loss: 5.2111, Val Loss: 5.1068\n",
      "Epoch 10/50, Train Loss: 5.2313, Val Loss: 5.3104\n",
      "Epoch 11/50, Train Loss: 5.1971, Val Loss: 5.3014\n",
      "Epoch 12/50, Train Loss: 5.2064, Val Loss: 5.1765\n",
      "Epoch 13/50, Train Loss: 5.1772, Val Loss: 5.1983\n",
      "Epoch 14/50, Train Loss: 5.1324, Val Loss: 5.1906\n",
      "Epoch 15/50, Train Loss: 5.0728, Val Loss: 5.0609\n",
      "Epoch 16/50, Train Loss: 5.2212, Val Loss: 5.1338\n",
      "Epoch 17/50, Train Loss: 5.0866, Val Loss: 5.2005\n",
      "Epoch 18/50, Train Loss: 5.1171, Val Loss: 5.2226\n",
      "Epoch 19/50, Train Loss: 5.0169, Val Loss: 5.0081\n",
      "Epoch 20/50, Train Loss: 5.0080, Val Loss: 5.0810\n",
      "Epoch 21/50, Train Loss: 5.0093, Val Loss: 4.9780\n",
      "Epoch 22/50, Train Loss: 5.0246, Val Loss: 4.9946\n",
      "Epoch 23/50, Train Loss: 4.9771, Val Loss: 5.0038\n",
      "Epoch 24/50, Train Loss: 5.1983, Val Loss: 5.0651\n",
      "Epoch 25/50, Train Loss: 5.0156, Val Loss: 5.0576\n",
      "Epoch 26/50, Train Loss: 5.0412, Val Loss: 5.1133\n",
      "Epoch 27/50, Train Loss: 4.9906, Val Loss: 5.1312\n",
      "Epoch 28/50, Train Loss: 4.9915, Val Loss: 4.9818\n",
      "Epoch 29/50, Train Loss: 5.0450, Val Loss: 5.0553\n",
      "Epoch 30/50, Train Loss: 4.9718, Val Loss: 5.1416\n",
      "Epoch 31/50, Train Loss: 5.0982, Val Loss: 5.0008\n",
      "Epoch 32/50, Train Loss: 4.9854, Val Loss: 5.0126\n",
      "Epoch 33/50, Train Loss: 5.0716, Val Loss: 4.9674\n",
      "Epoch 34/50, Train Loss: 4.9762, Val Loss: 5.0790\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9718, val_loss=4.9674, arbitrages_proportion=0.4672\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 50, 'patience': 20, 'lambda_butterfly': 10000.0}\n",
      "Epoch 1/50, Train Loss: 15.8719, Val Loss: 8.6737\n",
      "Epoch 2/50, Train Loss: 8.5095, Val Loss: 8.2464\n",
      "Epoch 3/50, Train Loss: 8.0933, Val Loss: 7.8421\n",
      "Epoch 4/50, Train Loss: 7.7072, Val Loss: 7.5626\n",
      "Epoch 5/50, Train Loss: 7.2491, Val Loss: 7.1311\n",
      "Epoch 6/50, Train Loss: 6.8961, Val Loss: 6.8181\n",
      "Epoch 7/50, Train Loss: 6.5519, Val Loss: 6.4044\n",
      "Epoch 8/50, Train Loss: 5.7181, Val Loss: 5.4703\n",
      "Epoch 9/50, Train Loss: 5.3054, Val Loss: 5.3070\n",
      "Epoch 10/50, Train Loss: 5.3300, Val Loss: 5.4070\n",
      "Epoch 11/50, Train Loss: 5.1914, Val Loss: 5.2135\n",
      "Epoch 12/50, Train Loss: 5.0970, Val Loss: 5.1369\n",
      "Epoch 13/50, Train Loss: 5.0889, Val Loss: 5.1594\n",
      "Epoch 14/50, Train Loss: 5.0644, Val Loss: 5.1191\n",
      "Epoch 15/50, Train Loss: 5.1028, Val Loss: 5.4443\n",
      "Epoch 16/50, Train Loss: 5.1785, Val Loss: 5.1485\n",
      "Epoch 17/50, Train Loss: 5.0665, Val Loss: 5.0732\n",
      "Epoch 18/50, Train Loss: 5.0739, Val Loss: 5.0814\n",
      "Epoch 19/50, Train Loss: 5.0647, Val Loss: 5.0873\n",
      "Epoch 20/50, Train Loss: 5.0591, Val Loss: 5.2188\n",
      "Epoch 21/50, Train Loss: 5.0863, Val Loss: 5.0856\n",
      "Epoch 22/50, Train Loss: 5.0158, Val Loss: 5.0573\n",
      "Epoch 23/50, Train Loss: 5.0216, Val Loss: 5.0396\n",
      "Epoch 24/50, Train Loss: 5.0727, Val Loss: 5.2938\n",
      "Epoch 25/50, Train Loss: 5.1587, Val Loss: 5.1579\n",
      "Epoch 26/50, Train Loss: 5.0876, Val Loss: 5.0538\n",
      "Epoch 27/50, Train Loss: 5.0159, Val Loss: 4.9974\n",
      "Epoch 28/50, Train Loss: 5.0167, Val Loss: 5.0666\n",
      "Epoch 29/50, Train Loss: 5.1397, Val Loss: 5.1190\n",
      "Epoch 30/50, Train Loss: 5.2081, Val Loss: 5.1168\n",
      "Epoch 31/50, Train Loss: 5.1807, Val Loss: 5.0787\n",
      "Epoch 32/50, Train Loss: 5.0482, Val Loss: 5.0498\n",
      "Epoch 33/50, Train Loss: 5.0399, Val Loss: 5.0362\n",
      "Epoch 34/50, Train Loss: 5.1370, Val Loss: 5.1918\n",
      "Epoch 35/50, Train Loss: 5.0121, Val Loss: 5.0277\n",
      "Epoch 36/50, Train Loss: 5.0059, Val Loss: 5.0024\n",
      "Epoch 37/50, Train Loss: 5.0375, Val Loss: 5.1159\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=5.0059, val_loss=4.9974, arbitrages_proportion=0.2041\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 100, 'patience': 10, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/100, Train Loss: 15.8591, Val Loss: 8.8928\n",
      "Epoch 2/100, Train Loss: 8.6724, Val Loss: 8.5980\n",
      "Epoch 3/100, Train Loss: 8.4049, Val Loss: 8.1007\n",
      "Epoch 4/100, Train Loss: 7.9248, Val Loss: 7.5781\n",
      "Epoch 5/100, Train Loss: 7.3270, Val Loss: 6.9598\n",
      "Epoch 6/100, Train Loss: 6.5763, Val Loss: 6.1507\n",
      "Epoch 7/100, Train Loss: 6.2151, Val Loss: 5.8753\n",
      "Epoch 8/100, Train Loss: 5.5451, Val Loss: 5.4412\n",
      "Epoch 9/100, Train Loss: 5.2775, Val Loss: 5.2384\n",
      "Epoch 10/100, Train Loss: 5.1867, Val Loss: 5.1628\n",
      "Epoch 11/100, Train Loss: 5.2211, Val Loss: 5.2307\n",
      "Epoch 12/100, Train Loss: 5.1516, Val Loss: 5.1612\n",
      "Epoch 13/100, Train Loss: 5.2675, Val Loss: 5.1549\n",
      "Epoch 14/100, Train Loss: 5.1695, Val Loss: 5.2238\n",
      "Epoch 15/100, Train Loss: 5.0879, Val Loss: 5.1747\n",
      "Epoch 16/100, Train Loss: 5.1353, Val Loss: 5.1762\n",
      "Epoch 17/100, Train Loss: 5.1083, Val Loss: 5.1581\n",
      "Epoch 18/100, Train Loss: 5.1078, Val Loss: 5.0903\n",
      "Epoch 19/100, Train Loss: 5.0592, Val Loss: 5.0637\n",
      "Epoch 20/100, Train Loss: 5.0787, Val Loss: 5.3312\n",
      "Epoch 21/100, Train Loss: 5.1066, Val Loss: 5.1204\n",
      "Epoch 22/100, Train Loss: 5.0742, Val Loss: 5.2826\n",
      "Epoch 23/100, Train Loss: 5.0724, Val Loss: 5.0914\n",
      "Epoch 24/100, Train Loss: 5.0431, Val Loss: 5.0462\n",
      "Epoch 25/100, Train Loss: 5.1192, Val Loss: 5.0993\n",
      "Epoch 26/100, Train Loss: 5.0565, Val Loss: 5.0945\n",
      "Epoch 27/100, Train Loss: 5.0404, Val Loss: 5.0833\n",
      "Epoch 28/100, Train Loss: 5.0584, Val Loss: 5.1938\n",
      "Epoch 29/100, Train Loss: 5.0939, Val Loss: 5.0900\n",
      "Epoch 30/100, Train Loss: 5.0609, Val Loss: 5.0443\n",
      "Epoch 31/100, Train Loss: 5.0695, Val Loss: 5.0985\n",
      "Epoch 32/100, Train Loss: 5.2706, Val Loss: 5.1767\n",
      "Epoch 33/100, Train Loss: 5.0602, Val Loss: 5.0243\n",
      "Epoch 34/100, Train Loss: 5.0380, Val Loss: 5.1356\n",
      "Epoch 35/100, Train Loss: 5.0256, Val Loss: 5.1259\n",
      "Epoch 36/100, Train Loss: 5.0041, Val Loss: 5.0440\n",
      "Epoch 37/100, Train Loss: 4.9895, Val Loss: 5.0576\n",
      "Epoch 38/100, Train Loss: 5.0239, Val Loss: 5.0879\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9895, val_loss=5.0243, arbitrages_proportion=0.2164\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 100, 'patience': 10, 'lambda_butterfly': 1000.0}\n",
      "Epoch 1/100, Train Loss: 16.0299, Val Loss: 8.7475\n",
      "Epoch 2/100, Train Loss: 8.4452, Val Loss: 8.0463\n",
      "Epoch 3/100, Train Loss: 7.6340, Val Loss: 7.0866\n",
      "Epoch 4/100, Train Loss: 6.5931, Val Loss: 5.8279\n",
      "Epoch 5/100, Train Loss: 5.5304, Val Loss: 5.3717\n",
      "Epoch 6/100, Train Loss: 5.4888, Val Loss: 5.4123\n",
      "Epoch 7/100, Train Loss: 5.3036, Val Loss: 5.2597\n",
      "Epoch 8/100, Train Loss: 5.2413, Val Loss: 5.2391\n",
      "Epoch 9/100, Train Loss: 5.1854, Val Loss: 5.1989\n",
      "Epoch 10/100, Train Loss: 5.2153, Val Loss: 5.2566\n",
      "Epoch 11/100, Train Loss: 5.1254, Val Loss: 5.1678\n",
      "Epoch 12/100, Train Loss: 5.1059, Val Loss: 5.1731\n",
      "Epoch 13/100, Train Loss: 5.1995, Val Loss: 5.3936\n",
      "Epoch 14/100, Train Loss: 5.1795, Val Loss: 5.1703\n",
      "Epoch 15/100, Train Loss: 5.0920, Val Loss: 5.0679\n",
      "Epoch 16/100, Train Loss: 5.0857, Val Loss: 5.1019\n",
      "Epoch 17/100, Train Loss: 5.0852, Val Loss: 5.1119\n",
      "Epoch 18/100, Train Loss: 5.0769, Val Loss: 5.1040\n",
      "Epoch 19/100, Train Loss: 5.0630, Val Loss: 5.1066\n",
      "Epoch 20/100, Train Loss: 5.0646, Val Loss: 5.1326\n",
      "Epoch 21/100, Train Loss: 5.1771, Val Loss: 5.0969\n",
      "Epoch 22/100, Train Loss: 5.0991, Val Loss: 5.0852\n",
      "Epoch 23/100, Train Loss: 5.0272, Val Loss: 5.0234\n",
      "Epoch 24/100, Train Loss: 5.0119, Val Loss: 5.0968\n",
      "Epoch 25/100, Train Loss: 5.0554, Val Loss: 5.1190\n",
      "Epoch 26/100, Train Loss: 5.0127, Val Loss: 5.0839\n",
      "Epoch 27/100, Train Loss: 5.2538, Val Loss: 6.0950\n",
      "Epoch 28/100, Train Loss: 5.3527, Val Loss: 5.0895\n",
      "Epoch 29/100, Train Loss: 5.0588, Val Loss: 4.9933\n",
      "Epoch 30/100, Train Loss: 4.9980, Val Loss: 5.0140\n",
      "Epoch 31/100, Train Loss: 5.0604, Val Loss: 5.0476\n",
      "Epoch 32/100, Train Loss: 4.9759, Val Loss: 4.9971\n",
      "Epoch 33/100, Train Loss: 4.9959, Val Loss: 5.0687\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9759, val_loss=4.9933, arbitrages_proportion=0.2606\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 100, 'patience': 10, 'lambda_butterfly': 10000.0}\n",
      "Epoch 1/100, Train Loss: 15.8691, Val Loss: 8.6875\n",
      "Epoch 2/100, Train Loss: 8.4025, Val Loss: 7.9902\n",
      "Epoch 3/100, Train Loss: 7.6652, Val Loss: 7.1544\n",
      "Epoch 4/100, Train Loss: 6.9345, Val Loss: 6.5283\n",
      "Epoch 5/100, Train Loss: 6.2198, Val Loss: 6.0488\n",
      "Epoch 6/100, Train Loss: 5.7297, Val Loss: 5.5370\n",
      "Epoch 7/100, Train Loss: 5.3725, Val Loss: 5.2643\n",
      "Epoch 8/100, Train Loss: 5.2256, Val Loss: 5.3394\n",
      "Epoch 9/100, Train Loss: 5.1817, Val Loss: 5.2013\n",
      "Epoch 10/100, Train Loss: 5.1948, Val Loss: 5.3179\n",
      "Epoch 11/100, Train Loss: 5.2296, Val Loss: 5.3305\n",
      "Epoch 12/100, Train Loss: 5.1903, Val Loss: 5.1940\n",
      "Epoch 13/100, Train Loss: 5.2458, Val Loss: 5.1982\n",
      "Epoch 14/100, Train Loss: 5.1165, Val Loss: 5.1031\n",
      "Epoch 15/100, Train Loss: 5.0672, Val Loss: 5.1196\n",
      "Epoch 16/100, Train Loss: 5.1297, Val Loss: 5.3489\n",
      "Epoch 17/100, Train Loss: 5.1289, Val Loss: 5.1405\n",
      "Epoch 18/100, Train Loss: 5.0969, Val Loss: 5.1837\n",
      "Epoch 19/100, Train Loss: 5.0818, Val Loss: 5.1363\n",
      "Epoch 20/100, Train Loss: 5.0791, Val Loss: 5.1524\n",
      "Epoch 21/100, Train Loss: 5.0747, Val Loss: 5.0916\n",
      "Epoch 22/100, Train Loss: 5.2229, Val Loss: 5.0673\n",
      "Epoch 23/100, Train Loss: 5.0542, Val Loss: 5.1236\n",
      "Epoch 24/100, Train Loss: 5.1358, Val Loss: 5.1824\n",
      "Epoch 25/100, Train Loss: 5.2239, Val Loss: 5.2051\n",
      "Epoch 26/100, Train Loss: 5.1430, Val Loss: 5.1017\n",
      "Epoch 27/100, Train Loss: 5.0794, Val Loss: 5.1123\n",
      "Epoch 28/100, Train Loss: 5.0660, Val Loss: 5.0608\n",
      "Epoch 29/100, Train Loss: 5.0187, Val Loss: 5.0389\n",
      "Epoch 30/100, Train Loss: 5.0575, Val Loss: 5.2385\n",
      "Epoch 31/100, Train Loss: 5.0912, Val Loss: 5.1236\n",
      "Epoch 32/100, Train Loss: 5.1027, Val Loss: 5.0822\n",
      "Epoch 33/100, Train Loss: 5.1463, Val Loss: 5.2297\n",
      "Epoch 34/100, Train Loss: 5.1139, Val Loss: 5.0737\n",
      "Epoch 35/100, Train Loss: 5.1383, Val Loss: 5.1549\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=5.0187, val_loss=5.0389, arbitrages_proportion=0.2858\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 100, 'patience': 20, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/100, Train Loss: 16.3941, Val Loss: 8.7181\n",
      "Epoch 2/100, Train Loss: 8.5794, Val Loss: 8.2325\n",
      "Epoch 3/100, Train Loss: 8.0733, Val Loss: 7.7887\n",
      "Epoch 4/100, Train Loss: 7.2588, Val Loss: 6.6999\n",
      "Epoch 5/100, Train Loss: 6.0369, Val Loss: 5.5627\n",
      "Epoch 6/100, Train Loss: 5.3648, Val Loss: 5.2979\n",
      "Epoch 7/100, Train Loss: 5.2877, Val Loss: 5.2339\n",
      "Epoch 8/100, Train Loss: 5.1729, Val Loss: 5.2217\n",
      "Epoch 9/100, Train Loss: 5.1854, Val Loss: 5.2161\n",
      "Epoch 10/100, Train Loss: 5.0963, Val Loss: 5.0955\n",
      "Epoch 11/100, Train Loss: 5.1345, Val Loss: 5.3893\n",
      "Epoch 12/100, Train Loss: 5.1575, Val Loss: 5.2050\n",
      "Epoch 13/100, Train Loss: 5.1330, Val Loss: 5.1579\n",
      "Epoch 14/100, Train Loss: 5.1756, Val Loss: 5.2160\n",
      "Epoch 15/100, Train Loss: 5.0890, Val Loss: 5.1094\n",
      "Epoch 16/100, Train Loss: 5.1398, Val Loss: 5.0979\n",
      "Epoch 17/100, Train Loss: 5.0997, Val Loss: 5.1289\n",
      "Epoch 18/100, Train Loss: 5.2142, Val Loss: 5.1687\n",
      "Epoch 19/100, Train Loss: 5.2575, Val Loss: 5.1800\n",
      "Epoch 20/100, Train Loss: 5.0445, Val Loss: 5.0570\n",
      "Epoch 21/100, Train Loss: 5.0447, Val Loss: 5.0295\n",
      "Epoch 22/100, Train Loss: 5.0208, Val Loss: 5.0192\n",
      "Epoch 23/100, Train Loss: 5.0222, Val Loss: 5.0952\n",
      "Epoch 24/100, Train Loss: 5.1092, Val Loss: 5.0452\n",
      "Epoch 25/100, Train Loss: 5.0540, Val Loss: 5.0434\n",
      "Epoch 26/100, Train Loss: 4.9889, Val Loss: 5.0902\n",
      "Epoch 27/100, Train Loss: 5.0142, Val Loss: 5.1259\n",
      "Epoch 28/100, Train Loss: 5.0256, Val Loss: 4.9996\n",
      "Epoch 29/100, Train Loss: 5.0136, Val Loss: 5.0136\n",
      "Epoch 30/100, Train Loss: 4.9710, Val Loss: 4.9596\n",
      "Epoch 31/100, Train Loss: 4.9850, Val Loss: 5.0142\n",
      "Epoch 32/100, Train Loss: 4.9718, Val Loss: 4.9748\n",
      "Epoch 33/100, Train Loss: 4.9589, Val Loss: 5.0464\n",
      "Epoch 34/100, Train Loss: 4.9563, Val Loss: 4.9697\n",
      "Epoch 35/100, Train Loss: 4.9692, Val Loss: 4.9782\n",
      "Epoch 36/100, Train Loss: 4.9603, Val Loss: 4.9885\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9563, val_loss=4.9596, arbitrages_proportion=0.1551\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 100, 'patience': 20, 'lambda_butterfly': 1000.0}\n",
      "Epoch 1/100, Train Loss: 15.0516, Val Loss: 8.6825\n",
      "Epoch 2/100, Train Loss: 8.4244, Val Loss: 8.0566\n",
      "Epoch 3/100, Train Loss: 7.8538, Val Loss: 7.4847\n",
      "Epoch 4/100, Train Loss: 7.1223, Val Loss: 7.0725\n",
      "Epoch 5/100, Train Loss: 6.3815, Val Loss: 6.0374\n",
      "Epoch 6/100, Train Loss: 5.7052, Val Loss: 5.4452\n",
      "Epoch 7/100, Train Loss: 5.3473, Val Loss: 5.2915\n",
      "Epoch 8/100, Train Loss: 5.3024, Val Loss: 5.2944\n",
      "Epoch 9/100, Train Loss: 5.2293, Val Loss: 5.2231\n",
      "Epoch 10/100, Train Loss: 5.2604, Val Loss: 5.4246\n",
      "Epoch 11/100, Train Loss: 5.2681, Val Loss: 5.1932\n",
      "Epoch 12/100, Train Loss: 5.3556, Val Loss: 5.4787\n",
      "Epoch 13/100, Train Loss: 5.1965, Val Loss: 5.1975\n",
      "Epoch 14/100, Train Loss: 5.1104, Val Loss: 5.1237\n",
      "Epoch 15/100, Train Loss: 5.1425, Val Loss: 5.1753\n",
      "Epoch 16/100, Train Loss: 5.0967, Val Loss: 5.0680\n",
      "Epoch 17/100, Train Loss: 5.0980, Val Loss: 5.1101\n",
      "Epoch 18/100, Train Loss: 5.1135, Val Loss: 5.1380\n",
      "Epoch 19/100, Train Loss: 5.1161, Val Loss: 5.1929\n",
      "Epoch 20/100, Train Loss: 5.0552, Val Loss: 5.0792\n",
      "Epoch 21/100, Train Loss: 5.1217, Val Loss: 5.1503\n",
      "Epoch 22/100, Train Loss: 5.0767, Val Loss: 5.0739\n",
      "Epoch 23/100, Train Loss: 5.0670, Val Loss: 5.0705\n",
      "Epoch 24/100, Train Loss: 5.0503, Val Loss: 5.0849\n",
      "Epoch 25/100, Train Loss: 4.9931, Val Loss: 5.0272\n",
      "Epoch 26/100, Train Loss: 5.0585, Val Loss: 5.2268\n",
      "Epoch 27/100, Train Loss: 5.0167, Val Loss: 5.0475\n",
      "Epoch 28/100, Train Loss: 5.0181, Val Loss: 5.0561\n",
      "Epoch 29/100, Train Loss: 5.0098, Val Loss: 5.0665\n",
      "Epoch 30/100, Train Loss: 5.0427, Val Loss: 5.1209\n",
      "Epoch 31/100, Train Loss: 5.0163, Val Loss: 5.1805\n",
      "Epoch 32/100, Train Loss: 5.0152, Val Loss: 5.0207\n",
      "Epoch 33/100, Train Loss: 4.9869, Val Loss: 5.0391\n",
      "Epoch 34/100, Train Loss: 5.0260, Val Loss: 5.0755\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9869, val_loss=5.0207, arbitrages_proportion=0.3807\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 64, 'epochs': 100, 'patience': 20, 'lambda_butterfly': 10000.0}\n",
      "Epoch 1/100, Train Loss: 15.6545, Val Loss: 8.5224\n",
      "Epoch 2/100, Train Loss: 8.3701, Val Loss: 7.8142\n",
      "Epoch 3/100, Train Loss: 7.7636, Val Loss: 7.2853\n",
      "Epoch 4/100, Train Loss: 6.7340, Val Loss: 6.2791\n",
      "Epoch 5/100, Train Loss: 6.1494, Val Loss: 5.9380\n",
      "Epoch 6/100, Train Loss: 5.9156, Val Loss: 5.7617\n",
      "Epoch 7/100, Train Loss: 5.6675, Val Loss: 5.4644\n",
      "Epoch 8/100, Train Loss: 5.2577, Val Loss: 5.4308\n",
      "Epoch 9/100, Train Loss: 5.2907, Val Loss: 5.2823\n",
      "Epoch 10/100, Train Loss: 5.2298, Val Loss: 5.2843\n",
      "Epoch 11/100, Train Loss: 5.1895, Val Loss: 5.1975\n",
      "Epoch 12/100, Train Loss: 5.1566, Val Loss: 5.1763\n",
      "Epoch 13/100, Train Loss: 5.1599, Val Loss: 5.1660\n",
      "Epoch 14/100, Train Loss: 5.1106, Val Loss: 5.1364\n",
      "Epoch 15/100, Train Loss: 5.0725, Val Loss: 5.1557\n",
      "Epoch 16/100, Train Loss: 5.0853, Val Loss: 5.1119\n",
      "Epoch 17/100, Train Loss: 5.0468, Val Loss: 5.0932\n",
      "Epoch 18/100, Train Loss: 5.0805, Val Loss: 5.1125\n",
      "Epoch 19/100, Train Loss: 5.0825, Val Loss: 5.1492\n",
      "Epoch 20/100, Train Loss: 5.2202, Val Loss: 5.3747\n",
      "Epoch 21/100, Train Loss: 5.1432, Val Loss: 5.0673\n",
      "Epoch 22/100, Train Loss: 5.1222, Val Loss: 4.9942\n",
      "Epoch 23/100, Train Loss: 5.0434, Val Loss: 5.0929\n",
      "Epoch 24/100, Train Loss: 5.1827, Val Loss: 5.0976\n",
      "Epoch 25/100, Train Loss: 5.0627, Val Loss: 5.0989\n",
      "Epoch 26/100, Train Loss: 5.0334, Val Loss: 5.0672\n",
      "Epoch 27/100, Train Loss: 5.0576, Val Loss: 5.1176\n",
      "Epoch 28/100, Train Loss: 5.0356, Val Loss: 5.1011\n",
      "Epoch 29/100, Train Loss: 5.0288, Val Loss: 5.0418\n",
      "Epoch 30/100, Train Loss: 5.0477, Val Loss: 5.0925\n",
      "Epoch 31/100, Train Loss: 5.0599, Val Loss: 5.0521\n",
      "Epoch 32/100, Train Loss: 5.1384, Val Loss: 5.1081\n",
      "Epoch 33/100, Train Loss: 5.0226, Val Loss: 5.1881\n",
      "Epoch 34/100, Train Loss: 5.0599, Val Loss: 5.0732\n",
      "Epoch 35/100, Train Loss: 5.0373, Val Loss: 5.0610\n",
      "Epoch 36/100, Train Loss: 5.0112, Val Loss: 5.0887\n",
      "Epoch 37/100, Train Loss: 4.9883, Val Loss: 5.0423\n",
      "Epoch 38/100, Train Loss: 5.0103, Val Loss: 5.1214\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9883, val_loss=4.9942, arbitrages_proportion=0.3108\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/50, Train Loss: 19.7945, Val Loss: 14.3675\n",
      "Epoch 2/50, Train Loss: 9.7126, Val Loss: 8.5598\n",
      "Epoch 3/50, Train Loss: 8.3769, Val Loss: 8.2359\n",
      "Epoch 4/50, Train Loss: 8.0739, Val Loss: 7.7609\n",
      "Epoch 5/50, Train Loss: 7.5582, Val Loss: 7.2392\n",
      "Epoch 6/50, Train Loss: 7.0307, Val Loss: 6.8441\n",
      "Epoch 7/50, Train Loss: 6.5791, Val Loss: 6.4966\n",
      "Epoch 8/50, Train Loss: 6.2397, Val Loss: 6.0893\n",
      "Epoch 9/50, Train Loss: 5.6802, Val Loss: 5.3834\n",
      "Epoch 10/50, Train Loss: 5.2958, Val Loss: 5.2976\n",
      "Epoch 11/50, Train Loss: 5.1998, Val Loss: 5.1698\n",
      "Epoch 12/50, Train Loss: 5.1669, Val Loss: 5.2390\n",
      "Epoch 13/50, Train Loss: 5.1916, Val Loss: 5.1499\n",
      "Epoch 14/50, Train Loss: 5.1911, Val Loss: 5.4778\n",
      "Epoch 15/50, Train Loss: 5.1815, Val Loss: 5.1629\n",
      "Epoch 16/50, Train Loss: 5.1266, Val Loss: 5.1746\n",
      "Epoch 17/50, Train Loss: 5.0970, Val Loss: 5.0793\n",
      "Epoch 18/50, Train Loss: 5.0745, Val Loss: 5.1325\n",
      "Epoch 19/50, Train Loss: 5.0965, Val Loss: 5.1261\n",
      "Epoch 20/50, Train Loss: 5.0964, Val Loss: 5.1605\n",
      "Epoch 21/50, Train Loss: 5.1459, Val Loss: 5.1489\n",
      "Epoch 22/50, Train Loss: 5.0892, Val Loss: 5.0852\n",
      "Epoch 23/50, Train Loss: 5.0604, Val Loss: 5.1020\n",
      "Epoch 24/50, Train Loss: 5.0449, Val Loss: 5.0909\n",
      "Epoch 25/50, Train Loss: 5.0294, Val Loss: 5.0857\n",
      "Epoch 26/50, Train Loss: 5.0136, Val Loss: 5.0476\n",
      "Epoch 27/50, Train Loss: 5.0081, Val Loss: 5.0892\n",
      "Epoch 28/50, Train Loss: 5.0048, Val Loss: 5.0364\n",
      "Epoch 29/50, Train Loss: 5.0327, Val Loss: 5.0524\n",
      "Epoch 30/50, Train Loss: 4.9948, Val Loss: 4.9959\n",
      "Epoch 31/50, Train Loss: 4.9766, Val Loss: 5.0177\n",
      "Epoch 32/50, Train Loss: 5.1760, Val Loss: 5.6097\n",
      "Epoch 33/50, Train Loss: 5.2798, Val Loss: 5.1501\n",
      "Epoch 34/50, Train Loss: 5.0498, Val Loss: 5.0789\n",
      "Epoch 35/50, Train Loss: 5.0246, Val Loss: 5.1066\n",
      "Epoch 36/50, Train Loss: 5.0111, Val Loss: 5.0332\n",
      "Epoch 37/50, Train Loss: 5.0303, Val Loss: 4.9950\n",
      "Epoch 38/50, Train Loss: 5.0073, Val Loss: 4.9977\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9766, val_loss=4.9950, arbitrages_proportion=0.1994\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 1000.0}\n",
      "Epoch 1/50, Train Loss: 18.8686, Val Loss: 10.7292\n",
      "Epoch 2/50, Train Loss: 8.9767, Val Loss: 8.6546\n",
      "Epoch 3/50, Train Loss: 8.4563, Val Loss: 8.3487\n",
      "Epoch 4/50, Train Loss: 8.2105, Val Loss: 7.9851\n",
      "Epoch 5/50, Train Loss: 7.8689, Val Loss: 7.4894\n",
      "Epoch 6/50, Train Loss: 7.3381, Val Loss: 7.0360\n",
      "Epoch 7/50, Train Loss: 6.7464, Val Loss: 6.5064\n",
      "Epoch 8/50, Train Loss: 6.2708, Val Loss: 6.0954\n",
      "Epoch 9/50, Train Loss: 5.9538, Val Loss: 5.8381\n",
      "Epoch 10/50, Train Loss: 5.6402, Val Loss: 5.4209\n",
      "Epoch 11/50, Train Loss: 5.3029, Val Loss: 5.2565\n",
      "Epoch 12/50, Train Loss: 5.2259, Val Loss: 5.2345\n",
      "Epoch 13/50, Train Loss: 5.1831, Val Loss: 5.1851\n",
      "Epoch 14/50, Train Loss: 5.1688, Val Loss: 5.2441\n",
      "Epoch 15/50, Train Loss: 5.1343, Val Loss: 5.1861\n",
      "Epoch 16/50, Train Loss: 5.1577, Val Loss: 5.2136\n",
      "Epoch 17/50, Train Loss: 5.1159, Val Loss: 5.2010\n",
      "Epoch 18/50, Train Loss: 5.1489, Val Loss: 5.1689\n",
      "Epoch 19/50, Train Loss: 5.0714, Val Loss: 5.0580\n",
      "Epoch 20/50, Train Loss: 5.1203, Val Loss: 5.2013\n",
      "Epoch 21/50, Train Loss: 5.0865, Val Loss: 5.1542\n",
      "Epoch 22/50, Train Loss: 5.0310, Val Loss: 5.1380\n",
      "Epoch 23/50, Train Loss: 5.0263, Val Loss: 5.0877\n",
      "Epoch 24/50, Train Loss: 5.0492, Val Loss: 4.9638\n",
      "Epoch 25/50, Train Loss: 5.0926, Val Loss: 5.0790\n",
      "Epoch 26/50, Train Loss: 5.0104, Val Loss: 5.0740\n",
      "Epoch 27/50, Train Loss: 5.0104, Val Loss: 5.0362\n",
      "Epoch 28/50, Train Loss: 5.0117, Val Loss: 5.0419\n",
      "Epoch 29/50, Train Loss: 4.9941, Val Loss: 4.9769\n",
      "Epoch 30/50, Train Loss: 5.0209, Val Loss: 4.9699\n",
      "Epoch 31/50, Train Loss: 4.9860, Val Loss: 5.0518\n",
      "Epoch 32/50, Train Loss: 4.9708, Val Loss: 5.0179\n",
      "Epoch 33/50, Train Loss: 4.9331, Val Loss: 4.9844\n",
      "Epoch 34/50, Train Loss: 4.9202, Val Loss: 5.0196\n",
      "Epoch 35/50, Train Loss: 4.9370, Val Loss: 4.9389\n",
      "Epoch 36/50, Train Loss: 4.9484, Val Loss: 4.9809\n",
      "Epoch 37/50, Train Loss: 4.9760, Val Loss: 5.1645\n",
      "Epoch 38/50, Train Loss: 4.9944, Val Loss: 4.9427\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9202, val_loss=4.9389, arbitrages_proportion=0.0945\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 10000.0}\n",
      "Epoch 1/50, Train Loss: 18.7609, Val Loss: 10.0033\n",
      "Epoch 2/50, Train Loss: 8.9825, Val Loss: 8.4780\n",
      "Epoch 3/50, Train Loss: 8.5240, Val Loss: 8.4189\n",
      "Epoch 4/50, Train Loss: 8.0577, Val Loss: 7.6666\n",
      "Epoch 5/50, Train Loss: 7.5427, Val Loss: 7.3183\n",
      "Epoch 6/50, Train Loss: 6.8881, Val Loss: 6.5494\n",
      "Epoch 7/50, Train Loss: 6.2255, Val Loss: 5.9307\n",
      "Epoch 8/50, Train Loss: 5.7692, Val Loss: 5.6019\n",
      "Epoch 9/50, Train Loss: 5.6410, Val Loss: 5.5506\n",
      "Epoch 10/50, Train Loss: 5.3595, Val Loss: 5.3946\n",
      "Epoch 11/50, Train Loss: 5.2083, Val Loss: 5.2889\n",
      "Epoch 12/50, Train Loss: 5.2171, Val Loss: 5.1973\n",
      "Epoch 13/50, Train Loss: 5.2165, Val Loss: 5.2253\n",
      "Epoch 14/50, Train Loss: 5.1657, Val Loss: 5.2423\n",
      "Epoch 15/50, Train Loss: 5.1642, Val Loss: 5.2067\n",
      "Epoch 16/50, Train Loss: 5.1118, Val Loss: 5.1620\n",
      "Epoch 17/50, Train Loss: 5.1337, Val Loss: 5.2191\n",
      "Epoch 18/50, Train Loss: 5.1005, Val Loss: 5.0877\n",
      "Epoch 19/50, Train Loss: 5.0868, Val Loss: 5.1687\n",
      "Epoch 20/50, Train Loss: 5.3628, Val Loss: 5.1898\n",
      "Epoch 21/50, Train Loss: 5.0936, Val Loss: 5.1253\n",
      "Epoch 22/50, Train Loss: 5.1006, Val Loss: 5.0963\n",
      "Epoch 23/50, Train Loss: 5.1108, Val Loss: 5.1339\n",
      "Epoch 24/50, Train Loss: 5.0707, Val Loss: 5.1375\n",
      "Epoch 25/50, Train Loss: 5.0536, Val Loss: 5.0314\n",
      "Epoch 26/50, Train Loss: 5.0846, Val Loss: 5.1172\n",
      "Epoch 27/50, Train Loss: 5.0453, Val Loss: 5.1012\n",
      "Epoch 28/50, Train Loss: 5.0524, Val Loss: 5.0488\n",
      "Epoch 29/50, Train Loss: 5.0259, Val Loss: 5.1231\n",
      "Epoch 30/50, Train Loss: 5.0563, Val Loss: 5.0971\n",
      "Epoch 31/50, Train Loss: 5.0631, Val Loss: 5.0670\n",
      "Epoch 32/50, Train Loss: 5.0038, Val Loss: 5.0935\n",
      "Epoch 33/50, Train Loss: 5.0191, Val Loss: 4.9867\n",
      "Epoch 34/50, Train Loss: 5.0092, Val Loss: 5.0581\n",
      "Epoch 35/50, Train Loss: 5.0555, Val Loss: 5.0207\n",
      "Epoch 36/50, Train Loss: 5.2744, Val Loss: 5.0601\n",
      "Epoch 37/50, Train Loss: 5.0593, Val Loss: 5.0133\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=5.0038, val_loss=4.9867, arbitrages_proportion=0.2410\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 50, 'patience': 20, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/50, Train Loss: 20.0753, Val Loss: 13.7457\n",
      "Epoch 2/50, Train Loss: 9.2554, Val Loss: 8.3905\n",
      "Epoch 3/50, Train Loss: 8.2058, Val Loss: 7.9868\n",
      "Epoch 4/50, Train Loss: 7.7238, Val Loss: 7.4997\n",
      "Epoch 5/50, Train Loss: 7.0762, Val Loss: 6.7121\n",
      "Epoch 6/50, Train Loss: 6.3873, Val Loss: 6.1231\n",
      "Epoch 7/50, Train Loss: 5.8541, Val Loss: 5.5484\n",
      "Epoch 8/50, Train Loss: 5.4146, Val Loss: 5.3048\n",
      "Epoch 9/50, Train Loss: 5.2444, Val Loss: 5.2537\n",
      "Epoch 10/50, Train Loss: 5.1821, Val Loss: 5.1981\n",
      "Epoch 11/50, Train Loss: 5.1878, Val Loss: 5.2060\n",
      "Epoch 12/50, Train Loss: 5.1608, Val Loss: 5.1964\n",
      "Epoch 13/50, Train Loss: 5.1333, Val Loss: 5.2552\n",
      "Epoch 14/50, Train Loss: 5.1435, Val Loss: 5.2782\n",
      "Epoch 15/50, Train Loss: 5.1625, Val Loss: 5.1696\n",
      "Epoch 16/50, Train Loss: 5.0837, Val Loss: 5.1506\n",
      "Epoch 17/50, Train Loss: 5.0798, Val Loss: 5.1332\n",
      "Epoch 18/50, Train Loss: 5.0611, Val Loss: 5.0809\n",
      "Epoch 19/50, Train Loss: 5.0574, Val Loss: 5.0146\n",
      "Epoch 20/50, Train Loss: 5.0339, Val Loss: 5.0883\n",
      "Epoch 21/50, Train Loss: 5.0611, Val Loss: 5.0761\n",
      "Epoch 22/50, Train Loss: 5.0154, Val Loss: 5.0520\n",
      "Epoch 23/50, Train Loss: 5.0056, Val Loss: 5.0342\n",
      "Epoch 24/50, Train Loss: 4.9847, Val Loss: 5.0723\n",
      "Epoch 25/50, Train Loss: 4.9873, Val Loss: 4.9797\n",
      "Epoch 26/50, Train Loss: 5.0567, Val Loss: 5.0336\n",
      "Epoch 27/50, Train Loss: 5.0245, Val Loss: 5.0916\n",
      "Epoch 28/50, Train Loss: 5.0267, Val Loss: 5.0685\n",
      "Epoch 29/50, Train Loss: 4.9949, Val Loss: 5.0171\n",
      "Epoch 30/50, Train Loss: 4.9864, Val Loss: 4.9680\n",
      "Epoch 31/50, Train Loss: 4.9976, Val Loss: 4.9932\n",
      "Epoch 32/50, Train Loss: 4.9729, Val Loss: 4.9798\n",
      "Epoch 33/50, Train Loss: 4.9576, Val Loss: 4.9638\n",
      "Epoch 34/50, Train Loss: 4.9525, Val Loss: 4.9615\n",
      "Epoch 35/50, Train Loss: 4.9518, Val Loss: 4.9490\n",
      "Epoch 36/50, Train Loss: 4.9639, Val Loss: 4.9699\n",
      "Epoch 37/50, Train Loss: 4.9483, Val Loss: 4.9895\n",
      "Epoch 38/50, Train Loss: 4.9770, Val Loss: 4.9461\n",
      "Epoch 39/50, Train Loss: 5.0003, Val Loss: 5.2626\n",
      "Epoch 40/50, Train Loss: 5.0596, Val Loss: 4.9748\n",
      "Epoch 41/50, Train Loss: 5.0013, Val Loss: 5.1897\n",
      "Epoch 42/50, Train Loss: 5.0073, Val Loss: 5.0324\n",
      "Epoch 43/50, Train Loss: 5.0462, Val Loss: 4.9878\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9483, val_loss=4.9461, arbitrages_proportion=0.1532\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 50, 'patience': 20, 'lambda_butterfly': 1000.0}\n",
      "Epoch 1/50, Train Loss: 18.9812, Val Loss: 10.7225\n",
      "Epoch 2/50, Train Loss: 9.0598, Val Loss: 8.3832\n",
      "Epoch 3/50, Train Loss: 8.4727, Val Loss: 8.3038\n",
      "Epoch 4/50, Train Loss: 8.2671, Val Loss: 8.0521\n",
      "Epoch 5/50, Train Loss: 7.7128, Val Loss: 7.3378\n",
      "Epoch 6/50, Train Loss: 6.9668, Val Loss: 6.6717\n",
      "Epoch 7/50, Train Loss: 6.2646, Val Loss: 5.9182\n",
      "Epoch 8/50, Train Loss: 5.8123, Val Loss: 5.5496\n",
      "Epoch 9/50, Train Loss: 5.4285, Val Loss: 5.4023\n",
      "Epoch 10/50, Train Loss: 5.2512, Val Loss: 5.2386\n",
      "Epoch 11/50, Train Loss: 5.2221, Val Loss: 5.2197\n",
      "Epoch 12/50, Train Loss: 5.1530, Val Loss: 5.1546\n",
      "Epoch 13/50, Train Loss: 5.1331, Val Loss: 5.2223\n",
      "Epoch 14/50, Train Loss: 5.0993, Val Loss: 5.1634\n",
      "Epoch 15/50, Train Loss: 5.0933, Val Loss: 5.1595\n",
      "Epoch 16/50, Train Loss: 5.2649, Val Loss: 5.2317\n",
      "Epoch 17/50, Train Loss: 5.1147, Val Loss: 5.1883\n",
      "Epoch 18/50, Train Loss: 5.0717, Val Loss: 5.1397\n",
      "Epoch 19/50, Train Loss: 5.0416, Val Loss: 5.0481\n",
      "Epoch 20/50, Train Loss: 5.0514, Val Loss: 5.0860\n",
      "Epoch 21/50, Train Loss: 5.0382, Val Loss: 5.1195\n",
      "Epoch 22/50, Train Loss: 5.0384, Val Loss: 5.0995\n",
      "Epoch 23/50, Train Loss: 5.0502, Val Loss: 5.1188\n",
      "Epoch 24/50, Train Loss: 5.0217, Val Loss: 5.0522\n",
      "Epoch 25/50, Train Loss: 5.0239, Val Loss: 5.1495\n",
      "Epoch 26/50, Train Loss: 5.0533, Val Loss: 5.1591\n",
      "Epoch 27/50, Train Loss: 5.0722, Val Loss: 5.0861\n",
      "Epoch 28/50, Train Loss: 5.0279, Val Loss: 5.0375\n",
      "Epoch 29/50, Train Loss: 5.0082, Val Loss: 4.9968\n",
      "Epoch 30/50, Train Loss: 5.0335, Val Loss: 5.0674\n",
      "Epoch 31/50, Train Loss: 5.0079, Val Loss: 5.0912\n",
      "Epoch 32/50, Train Loss: 4.9952, Val Loss: 5.0539\n",
      "Epoch 33/50, Train Loss: 4.9928, Val Loss: 4.9839\n",
      "Epoch 34/50, Train Loss: 5.0026, Val Loss: 5.0229\n",
      "Epoch 35/50, Train Loss: 4.9806, Val Loss: 5.0442\n",
      "Epoch 36/50, Train Loss: 5.0184, Val Loss: 5.0903\n",
      "Epoch 37/50, Train Loss: 5.0107, Val Loss: 4.9846\n",
      "Epoch 38/50, Train Loss: 4.9669, Val Loss: 5.0065\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9669, val_loss=4.9839, arbitrages_proportion=0.0944\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 50, 'patience': 20, 'lambda_butterfly': 10000.0}\n",
      "Epoch 1/50, Train Loss: 19.8258, Val Loss: 14.4390\n",
      "Epoch 2/50, Train Loss: 9.7761, Val Loss: 8.5919\n",
      "Epoch 3/50, Train Loss: 8.4609, Val Loss: 8.4845\n",
      "Epoch 4/50, Train Loss: 8.2243, Val Loss: 8.1983\n",
      "Epoch 5/50, Train Loss: 7.9666, Val Loss: 7.7010\n",
      "Epoch 6/50, Train Loss: 7.6008, Val Loss: 7.3991\n",
      "Epoch 7/50, Train Loss: 7.1748, Val Loss: 6.8438\n",
      "Epoch 8/50, Train Loss: 6.6075, Val Loss: 6.2639\n",
      "Epoch 9/50, Train Loss: 5.8371, Val Loss: 5.5666\n",
      "Epoch 10/50, Train Loss: 5.3779, Val Loss: 5.3972\n",
      "Epoch 11/50, Train Loss: 5.2618, Val Loss: 5.2682\n",
      "Epoch 12/50, Train Loss: 5.1936, Val Loss: 5.1906\n",
      "Epoch 13/50, Train Loss: 5.1456, Val Loss: 5.1915\n",
      "Epoch 14/50, Train Loss: 5.1682, Val Loss: 5.1542\n",
      "Epoch 15/50, Train Loss: 5.1614, Val Loss: 5.1370\n",
      "Epoch 16/50, Train Loss: 5.1026, Val Loss: 5.1270\n",
      "Epoch 17/50, Train Loss: 5.0726, Val Loss: 5.0431\n",
      "Epoch 18/50, Train Loss: 5.0864, Val Loss: 5.2302\n",
      "Epoch 19/50, Train Loss: 5.0819, Val Loss: 5.0746\n",
      "Epoch 20/50, Train Loss: 5.0750, Val Loss: 5.1127\n",
      "Epoch 21/50, Train Loss: 5.0384, Val Loss: 5.0920\n",
      "Epoch 22/50, Train Loss: 5.0297, Val Loss: 5.1276\n",
      "Epoch 23/50, Train Loss: 5.0446, Val Loss: 5.0529\n",
      "Epoch 24/50, Train Loss: 4.9997, Val Loss: 5.0197\n",
      "Epoch 25/50, Train Loss: 4.9928, Val Loss: 5.0604\n",
      "Epoch 26/50, Train Loss: 4.9842, Val Loss: 4.9698\n",
      "Epoch 27/50, Train Loss: 5.0036, Val Loss: 5.0078\n",
      "Epoch 28/50, Train Loss: 4.9599, Val Loss: 5.1006\n",
      "Epoch 29/50, Train Loss: 4.9700, Val Loss: 5.0381\n",
      "Epoch 30/50, Train Loss: 5.0348, Val Loss: 5.0366\n",
      "Epoch 31/50, Train Loss: 5.0007, Val Loss: 5.0445\n",
      "Epoch 32/50, Train Loss: 4.9940, Val Loss: 5.1171\n",
      "Epoch 33/50, Train Loss: 5.0013, Val Loss: 5.0430\n",
      "Epoch 34/50, Train Loss: 5.1032, Val Loss: 5.0111\n",
      "Epoch 35/50, Train Loss: 4.9733, Val Loss: 4.9505\n",
      "Epoch 36/50, Train Loss: 5.0115, Val Loss: 5.0034\n",
      "Epoch 37/50, Train Loss: 4.9437, Val Loss: 4.9696\n",
      "Epoch 38/50, Train Loss: 4.9336, Val Loss: 4.9886\n",
      "Epoch 39/50, Train Loss: 4.9476, Val Loss: 4.9709\n",
      "Epoch 40/50, Train Loss: 4.9765, Val Loss: 4.9926\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9336, val_loss=4.9505, arbitrages_proportion=0.3012\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 100, 'patience': 10, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/100, Train Loss: 19.0593, Val Loss: 10.4124\n",
      "Epoch 2/100, Train Loss: 8.9542, Val Loss: 8.4719\n",
      "Epoch 3/100, Train Loss: 8.3442, Val Loss: 8.0194\n",
      "Epoch 4/100, Train Loss: 8.1127, Val Loss: 7.8375\n",
      "Epoch 5/100, Train Loss: 7.6111, Val Loss: 7.3034\n",
      "Epoch 6/100, Train Loss: 7.0064, Val Loss: 6.6910\n",
      "Epoch 7/100, Train Loss: 6.2769, Val Loss: 6.0097\n",
      "Epoch 8/100, Train Loss: 5.6617, Val Loss: 5.4202\n",
      "Epoch 9/100, Train Loss: 5.3400, Val Loss: 5.3001\n",
      "Epoch 10/100, Train Loss: 5.4561, Val Loss: 5.3580\n",
      "Epoch 11/100, Train Loss: 5.2388, Val Loss: 5.2182\n",
      "Epoch 12/100, Train Loss: 5.1742, Val Loss: 5.1953\n",
      "Epoch 13/100, Train Loss: 5.1334, Val Loss: 5.1649\n",
      "Epoch 14/100, Train Loss: 5.1542, Val Loss: 5.2013\n",
      "Epoch 15/100, Train Loss: 5.1345, Val Loss: 5.2020\n",
      "Epoch 16/100, Train Loss: 5.1091, Val Loss: 5.1079\n",
      "Epoch 17/100, Train Loss: 5.1087, Val Loss: 5.1406\n",
      "Epoch 18/100, Train Loss: 5.0895, Val Loss: 5.1760\n",
      "Epoch 19/100, Train Loss: 5.0758, Val Loss: 5.1349\n",
      "Epoch 20/100, Train Loss: 5.0722, Val Loss: 5.1685\n",
      "Epoch 21/100, Train Loss: 5.0312, Val Loss: 5.1081\n",
      "Epoch 22/100, Train Loss: 5.0473, Val Loss: 5.0722\n",
      "Epoch 23/100, Train Loss: 5.0525, Val Loss: 5.0852\n",
      "Epoch 24/100, Train Loss: 5.0561, Val Loss: 5.0746\n",
      "Epoch 25/100, Train Loss: 5.0464, Val Loss: 5.0748\n",
      "Epoch 26/100, Train Loss: 4.9841, Val Loss: 5.0248\n",
      "Epoch 27/100, Train Loss: 5.0074, Val Loss: 5.0732\n",
      "Epoch 28/100, Train Loss: 5.0004, Val Loss: 5.0328\n",
      "Epoch 29/100, Train Loss: 4.9968, Val Loss: 5.0241\n",
      "Epoch 30/100, Train Loss: 5.0110, Val Loss: 5.0411\n",
      "Epoch 31/100, Train Loss: 4.9853, Val Loss: 5.0144\n",
      "Epoch 32/100, Train Loss: 4.9696, Val Loss: 5.0459\n",
      "Epoch 33/100, Train Loss: 4.9738, Val Loss: 5.0172\n",
      "Epoch 34/100, Train Loss: 4.9751, Val Loss: 5.0050\n",
      "Epoch 35/100, Train Loss: 4.9821, Val Loss: 5.0703\n",
      "Epoch 36/100, Train Loss: 5.0401, Val Loss: 5.0160\n",
      "Epoch 37/100, Train Loss: 5.0007, Val Loss: 5.2270\n",
      "Epoch 38/100, Train Loss: 4.9968, Val Loss: 5.0457\n",
      "Epoch 39/100, Train Loss: 4.9799, Val Loss: 5.0886\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9696, val_loss=5.0050, arbitrages_proportion=0.0733\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 100, 'patience': 10, 'lambda_butterfly': 1000.0}\n",
      "Epoch 1/100, Train Loss: 19.6897, Val Loss: 11.9475\n",
      "Epoch 2/100, Train Loss: 9.0511, Val Loss: 8.3632\n",
      "Epoch 3/100, Train Loss: 8.1669, Val Loss: 7.9481\n",
      "Epoch 4/100, Train Loss: 7.7190, Val Loss: 7.5514\n",
      "Epoch 5/100, Train Loss: 7.3034, Val Loss: 7.0272\n",
      "Epoch 6/100, Train Loss: 6.9366, Val Loss: 6.7410\n",
      "Epoch 7/100, Train Loss: 6.5828, Val Loss: 6.4525\n",
      "Epoch 8/100, Train Loss: 6.2799, Val Loss: 6.1185\n",
      "Epoch 9/100, Train Loss: 5.9463, Val Loss: 5.6912\n",
      "Epoch 10/100, Train Loss: 5.5193, Val Loss: 5.5018\n",
      "Epoch 11/100, Train Loss: 5.2919, Val Loss: 5.2742\n",
      "Epoch 12/100, Train Loss: 5.2183, Val Loss: 5.2303\n",
      "Epoch 13/100, Train Loss: 5.1700, Val Loss: 5.1649\n",
      "Epoch 14/100, Train Loss: 5.1459, Val Loss: 5.1478\n",
      "Epoch 15/100, Train Loss: 5.1312, Val Loss: 5.1859\n",
      "Epoch 16/100, Train Loss: 5.1212, Val Loss: 5.1416\n",
      "Epoch 17/100, Train Loss: 5.0871, Val Loss: 5.1129\n",
      "Epoch 18/100, Train Loss: 5.2154, Val Loss: 5.1331\n",
      "Epoch 19/100, Train Loss: 5.0849, Val Loss: 5.1139\n",
      "Epoch 20/100, Train Loss: 5.0474, Val Loss: 5.0680\n",
      "Epoch 21/100, Train Loss: 5.0512, Val Loss: 5.0626\n",
      "Epoch 22/100, Train Loss: 5.0552, Val Loss: 5.0549\n",
      "Epoch 23/100, Train Loss: 5.0510, Val Loss: 5.0345\n",
      "Epoch 24/100, Train Loss: 5.0351, Val Loss: 5.1308\n",
      "Epoch 25/100, Train Loss: 5.0423, Val Loss: 5.0598\n",
      "Epoch 26/100, Train Loss: 5.0306, Val Loss: 5.0385\n",
      "Epoch 27/100, Train Loss: 5.0381, Val Loss: 4.9881\n",
      "Epoch 28/100, Train Loss: 5.0334, Val Loss: 5.0426\n",
      "Epoch 29/100, Train Loss: 5.0275, Val Loss: 5.1503\n",
      "Epoch 30/100, Train Loss: 5.0461, Val Loss: 5.0472\n",
      "Epoch 31/100, Train Loss: 5.0800, Val Loss: 5.0419\n",
      "Epoch 32/100, Train Loss: 4.9902, Val Loss: 4.9281\n",
      "Epoch 33/100, Train Loss: 4.9832, Val Loss: 4.9801\n",
      "Epoch 34/100, Train Loss: 4.9664, Val Loss: 5.0095\n",
      "Epoch 35/100, Train Loss: 4.9548, Val Loss: 5.0192\n",
      "Epoch 36/100, Train Loss: 4.9881, Val Loss: 5.0350\n",
      "Epoch 37/100, Train Loss: 4.9472, Val Loss: 4.9610\n",
      "Epoch 38/100, Train Loss: 4.9701, Val Loss: 5.0121\n",
      "Epoch 39/100, Train Loss: 4.9488, Val Loss: 4.9311\n",
      "Epoch 40/100, Train Loss: 4.9840, Val Loss: 5.0348\n",
      "Epoch 41/100, Train Loss: 4.9808, Val Loss: 5.0013\n",
      "Epoch 42/100, Train Loss: 5.0482, Val Loss: 5.0980\n",
      "Epoch 43/100, Train Loss: 5.0024, Val Loss: 4.9886\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9472, val_loss=4.9281, arbitrages_proportion=0.1501\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 100, 'patience': 10, 'lambda_butterfly': 10000.0}\n",
      "Epoch 1/100, Train Loss: 18.6399, Val Loss: 10.0874\n",
      "Epoch 2/100, Train Loss: 8.9609, Val Loss: 8.5944\n",
      "Epoch 3/100, Train Loss: 8.5666, Val Loss: 8.4583\n",
      "Epoch 4/100, Train Loss: 8.4144, Val Loss: 8.3225\n",
      "Epoch 5/100, Train Loss: 8.0383, Val Loss: 7.6739\n",
      "Epoch 6/100, Train Loss: 7.3567, Val Loss: 6.8806\n",
      "Epoch 7/100, Train Loss: 6.3621, Val Loss: 5.8254\n",
      "Epoch 8/100, Train Loss: 5.6857, Val Loss: 5.4160\n",
      "Epoch 9/100, Train Loss: 5.3680, Val Loss: 5.3492\n",
      "Epoch 10/100, Train Loss: 5.2839, Val Loss: 5.3410\n",
      "Epoch 11/100, Train Loss: 5.3200, Val Loss: 5.3166\n",
      "Epoch 12/100, Train Loss: 5.2171, Val Loss: 5.1820\n",
      "Epoch 13/100, Train Loss: 5.1928, Val Loss: 5.2151\n",
      "Epoch 14/100, Train Loss: 5.1502, Val Loss: 5.2146\n",
      "Epoch 15/100, Train Loss: 5.1871, Val Loss: 5.3767\n",
      "Epoch 16/100, Train Loss: 5.2041, Val Loss: 5.1811\n",
      "Epoch 17/100, Train Loss: 5.1071, Val Loss: 5.2118\n",
      "Epoch 18/100, Train Loss: 5.1057, Val Loss: 5.1706\n",
      "Epoch 19/100, Train Loss: 5.1607, Val Loss: 5.3319\n",
      "Epoch 20/100, Train Loss: 5.1047, Val Loss: 5.1398\n",
      "Epoch 21/100, Train Loss: 5.0721, Val Loss: 5.0707\n",
      "Epoch 22/100, Train Loss: 5.0474, Val Loss: 5.0173\n",
      "Epoch 23/100, Train Loss: 5.0197, Val Loss: 5.0773\n",
      "Epoch 24/100, Train Loss: 5.0163, Val Loss: 5.0101\n",
      "Epoch 25/100, Train Loss: 5.0829, Val Loss: 5.0720\n",
      "Epoch 26/100, Train Loss: 5.2448, Val Loss: 5.2097\n",
      "Epoch 27/100, Train Loss: 5.0954, Val Loss: 5.0758\n",
      "Epoch 28/100, Train Loss: 5.0000, Val Loss: 5.0531\n",
      "Epoch 29/100, Train Loss: 4.9970, Val Loss: 5.0061\n",
      "Epoch 30/100, Train Loss: 5.0111, Val Loss: 5.0127\n",
      "Epoch 31/100, Train Loss: 4.9858, Val Loss: 5.0029\n",
      "Epoch 32/100, Train Loss: 4.9894, Val Loss: 5.0134\n",
      "Epoch 33/100, Train Loss: 4.9851, Val Loss: 5.1341\n",
      "Epoch 34/100, Train Loss: 4.9987, Val Loss: 5.0107\n",
      "Epoch 35/100, Train Loss: 4.9688, Val Loss: 5.0144\n",
      "Epoch 36/100, Train Loss: 4.9881, Val Loss: 5.0146\n",
      "Epoch 37/100, Train Loss: 4.9797, Val Loss: 5.0446\n",
      "Epoch 38/100, Train Loss: 4.9906, Val Loss: 5.0918\n",
      "Epoch 39/100, Train Loss: 4.9836, Val Loss: 5.1108\n",
      "Epoch 40/100, Train Loss: 5.0075, Val Loss: 5.0077\n",
      "Epoch 41/100, Train Loss: 4.9618, Val Loss: 4.9836\n",
      "Epoch 42/100, Train Loss: 4.9782, Val Loss: 5.0266\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9618, val_loss=4.9836, arbitrages_proportion=0.2563\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 100, 'patience': 20, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/100, Train Loss: 20.9641, Val Loss: 18.8223\n",
      "Epoch 2/100, Train Loss: 11.7948, Val Loss: 8.4437\n",
      "Epoch 3/100, Train Loss: 8.3264, Val Loss: 8.2505\n",
      "Epoch 4/100, Train Loss: 7.9954, Val Loss: 7.7263\n",
      "Epoch 5/100, Train Loss: 7.3799, Val Loss: 6.8992\n",
      "Epoch 6/100, Train Loss: 6.6482, Val Loss: 6.3041\n",
      "Epoch 7/100, Train Loss: 6.1923, Val Loss: 6.0251\n",
      "Epoch 8/100, Train Loss: 5.9495, Val Loss: 5.9790\n",
      "Epoch 9/100, Train Loss: 5.7670, Val Loss: 5.6515\n",
      "Epoch 10/100, Train Loss: 5.5167, Val Loss: 5.3860\n",
      "Epoch 11/100, Train Loss: 5.2955, Val Loss: 5.2385\n",
      "Epoch 12/100, Train Loss: 5.1972, Val Loss: 5.3689\n",
      "Epoch 13/100, Train Loss: 5.1963, Val Loss: 5.2062\n",
      "Epoch 14/100, Train Loss: 5.1268, Val Loss: 5.1934\n",
      "Epoch 15/100, Train Loss: 5.1444, Val Loss: 5.3293\n",
      "Epoch 16/100, Train Loss: 5.4116, Val Loss: 5.2463\n",
      "Epoch 17/100, Train Loss: 5.1236, Val Loss: 5.2040\n",
      "Epoch 18/100, Train Loss: 5.0927, Val Loss: 5.1177\n",
      "Epoch 19/100, Train Loss: 5.9289, Val Loss: 6.7828\n",
      "Epoch 20/100, Train Loss: 5.6576, Val Loss: 5.2893\n",
      "Epoch 21/100, Train Loss: 5.2134, Val Loss: 5.2251\n",
      "Epoch 22/100, Train Loss: 5.1416, Val Loss: 5.1054\n",
      "Epoch 23/100, Train Loss: 5.1205, Val Loss: 5.1247\n",
      "Epoch 24/100, Train Loss: 5.0690, Val Loss: 5.1438\n",
      "Epoch 25/100, Train Loss: 5.0629, Val Loss: 5.1151\n",
      "Epoch 26/100, Train Loss: 5.0693, Val Loss: 5.1013\n",
      "Epoch 27/100, Train Loss: 5.0518, Val Loss: 5.1209\n",
      "Epoch 28/100, Train Loss: 5.0441, Val Loss: 5.0519\n",
      "Epoch 29/100, Train Loss: 5.0272, Val Loss: 5.0453\n",
      "Epoch 30/100, Train Loss: 5.0668, Val Loss: 5.0869\n",
      "Epoch 31/100, Train Loss: 5.0350, Val Loss: 5.0526\n",
      "Epoch 32/100, Train Loss: 5.0067, Val Loss: 5.0237\n",
      "Epoch 33/100, Train Loss: 5.0022, Val Loss: 5.0513\n",
      "Epoch 34/100, Train Loss: 5.0060, Val Loss: 5.0070\n",
      "Epoch 35/100, Train Loss: 4.9904, Val Loss: 4.9955\n",
      "Epoch 36/100, Train Loss: 4.9722, Val Loss: 5.0240\n",
      "Epoch 37/100, Train Loss: 4.9900, Val Loss: 4.9848\n",
      "Epoch 38/100, Train Loss: 5.0002, Val Loss: 5.0448\n",
      "Epoch 39/100, Train Loss: 4.9660, Val Loss: 4.9965\n",
      "Epoch 40/100, Train Loss: 4.9749, Val Loss: 5.0381\n",
      "Epoch 41/100, Train Loss: 4.9616, Val Loss: 5.0360\n",
      "Epoch 42/100, Train Loss: 5.0080, Val Loss: 5.0648\n",
      "Epoch 43/100, Train Loss: 4.9801, Val Loss: 4.9883\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9616, val_loss=4.9848, arbitrages_proportion=0.1590\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 100, 'patience': 20, 'lambda_butterfly': 1000.0}\n",
      "Epoch 1/100, Train Loss: 19.5008, Val Loss: 14.2201\n",
      "Epoch 2/100, Train Loss: 9.8872, Val Loss: 8.5888\n",
      "Epoch 3/100, Train Loss: 8.4188, Val Loss: 8.2107\n",
      "Epoch 4/100, Train Loss: 8.0832, Val Loss: 7.8097\n",
      "Epoch 5/100, Train Loss: 7.8121, Val Loss: 7.5350\n",
      "Epoch 6/100, Train Loss: 7.4110, Val Loss: 7.0971\n",
      "Epoch 7/100, Train Loss: 6.9160, Val Loss: 6.6179\n",
      "Epoch 8/100, Train Loss: 6.3645, Val Loss: 6.0900\n",
      "Epoch 9/100, Train Loss: 6.0527, Val Loss: 5.9812\n",
      "Epoch 10/100, Train Loss: 6.0081, Val Loss: 5.9766\n",
      "Epoch 11/100, Train Loss: 5.8868, Val Loss: 5.8638\n",
      "Epoch 12/100, Train Loss: 5.7930, Val Loss: 5.8563\n",
      "Epoch 13/100, Train Loss: 5.6627, Val Loss: 5.6416\n",
      "Epoch 14/100, Train Loss: 5.4743, Val Loss: 5.4261\n",
      "Epoch 15/100, Train Loss: 5.2178, Val Loss: 5.1314\n",
      "Epoch 16/100, Train Loss: 5.1004, Val Loss: 5.1014\n",
      "Epoch 17/100, Train Loss: 5.1685, Val Loss: 5.2320\n",
      "Epoch 18/100, Train Loss: 5.1141, Val Loss: 5.0426\n",
      "Epoch 19/100, Train Loss: 5.0085, Val Loss: 5.0354\n",
      "Epoch 20/100, Train Loss: 5.0394, Val Loss: 5.1016\n",
      "Epoch 21/100, Train Loss: 5.0067, Val Loss: 4.9740\n",
      "Epoch 22/100, Train Loss: 4.9857, Val Loss: 5.0479\n",
      "Epoch 23/100, Train Loss: 4.9569, Val Loss: 4.8973\n",
      "Epoch 24/100, Train Loss: 4.9805, Val Loss: 4.9818\n",
      "Epoch 25/100, Train Loss: 4.9643, Val Loss: 4.9455\n",
      "Epoch 26/100, Train Loss: 4.9420, Val Loss: 4.9194\n",
      "Epoch 27/100, Train Loss: 4.9409, Val Loss: 4.9951\n",
      "Epoch 28/100, Train Loss: 4.9317, Val Loss: 4.9809\n",
      "Epoch 29/100, Train Loss: 4.9478, Val Loss: 4.9180\n",
      "Epoch 30/100, Train Loss: 4.9723, Val Loss: 5.0372\n",
      "Epoch 31/100, Train Loss: 4.9525, Val Loss: 5.0083\n",
      "Epoch 32/100, Train Loss: 4.9329, Val Loss: 4.8955\n",
      "Epoch 33/100, Train Loss: 4.9254, Val Loss: 5.0267\n",
      "Epoch 34/100, Train Loss: 4.9124, Val Loss: 4.9278\n",
      "Epoch 35/100, Train Loss: 4.8786, Val Loss: 4.9323\n",
      "Epoch 36/100, Train Loss: 4.9019, Val Loss: 4.9672\n",
      "Epoch 37/100, Train Loss: 4.9010, Val Loss: 4.9422\n",
      "Epoch 38/100, Train Loss: 4.9322, Val Loss: 4.9615\n",
      "Epoch 39/100, Train Loss: 4.9078, Val Loss: 4.9147\n",
      "Epoch 40/100, Train Loss: 4.8977, Val Loss: 4.9029\n",
      "Epoch 41/100, Train Loss: 4.9775, Val Loss: 5.0218\n",
      "Epoch 42/100, Train Loss: 4.9339, Val Loss: 4.9333\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.8786, val_loss=4.8955, arbitrages_proportion=0.1032\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 128, 'epochs': 100, 'patience': 20, 'lambda_butterfly': 10000.0}\n",
      "Epoch 1/100, Train Loss: 20.1769, Val Loss: 14.3653\n",
      "Epoch 2/100, Train Loss: 9.6219, Val Loss: 8.3739\n",
      "Epoch 3/100, Train Loss: 8.3684, Val Loss: 8.0864\n",
      "Epoch 4/100, Train Loss: 7.9102, Val Loss: 7.7582\n",
      "Epoch 5/100, Train Loss: 7.2983, Val Loss: 6.9802\n",
      "Epoch 6/100, Train Loss: 6.6734, Val Loss: 6.3381\n",
      "Epoch 7/100, Train Loss: 6.1158, Val Loss: 5.8504\n",
      "Epoch 8/100, Train Loss: 5.6204, Val Loss: 5.5038\n",
      "Epoch 9/100, Train Loss: 5.3451, Val Loss: 5.3108\n",
      "Epoch 10/100, Train Loss: 5.2435, Val Loss: 5.2534\n",
      "Epoch 11/100, Train Loss: 5.3359, Val Loss: 5.3238\n",
      "Epoch 12/100, Train Loss: 5.1934, Val Loss: 5.2553\n",
      "Epoch 13/100, Train Loss: 5.1362, Val Loss: 5.2405\n",
      "Epoch 14/100, Train Loss: 5.1770, Val Loss: 5.1585\n",
      "Epoch 15/100, Train Loss: 5.2039, Val Loss: 5.2818\n",
      "Epoch 16/100, Train Loss: 5.1763, Val Loss: 5.1976\n",
      "Epoch 17/100, Train Loss: 5.1148, Val Loss: 5.1771\n",
      "Epoch 18/100, Train Loss: 5.1319, Val Loss: 5.1374\n",
      "Epoch 19/100, Train Loss: 5.0567, Val Loss: 5.1201\n",
      "Epoch 20/100, Train Loss: 5.0575, Val Loss: 5.0547\n",
      "Epoch 21/100, Train Loss: 5.0544, Val Loss: 5.0791\n",
      "Epoch 22/100, Train Loss: 5.0362, Val Loss: 5.1355\n",
      "Epoch 23/100, Train Loss: 5.0585, Val Loss: 5.0822\n",
      "Epoch 24/100, Train Loss: 5.0526, Val Loss: 5.0962\n",
      "Epoch 25/100, Train Loss: 5.0251, Val Loss: 5.0469\n",
      "Epoch 26/100, Train Loss: 5.0443, Val Loss: 5.1285\n",
      "Epoch 27/100, Train Loss: 5.0228, Val Loss: 5.0532\n",
      "Epoch 28/100, Train Loss: 5.0039, Val Loss: 5.0342\n",
      "Epoch 29/100, Train Loss: 5.1109, Val Loss: 5.1942\n",
      "Epoch 30/100, Train Loss: 5.0683, Val Loss: 5.0776\n",
      "Epoch 31/100, Train Loss: 4.9898, Val Loss: 5.0623\n",
      "Epoch 32/100, Train Loss: 4.9809, Val Loss: 5.0287\n",
      "Epoch 33/100, Train Loss: 5.0288, Val Loss: 5.0129\n",
      "Epoch 34/100, Train Loss: 4.9788, Val Loss: 5.0197\n",
      "Epoch 35/100, Train Loss: 4.9687, Val Loss: 5.0688\n",
      "Epoch 36/100, Train Loss: 4.9781, Val Loss: 5.0060\n",
      "Epoch 37/100, Train Loss: 4.9534, Val Loss: 4.9688\n",
      "Epoch 38/100, Train Loss: 4.9552, Val Loss: 4.9894\n",
      "Epoch 39/100, Train Loss: 4.9699, Val Loss: 4.9142\n",
      "Epoch 40/100, Train Loss: 4.9642, Val Loss: 5.0050\n",
      "Epoch 41/100, Train Loss: 4.9940, Val Loss: 5.2273\n",
      "Epoch 42/100, Train Loss: 5.0687, Val Loss: 4.9852\n",
      "Epoch 43/100, Train Loss: 4.9564, Val Loss: 5.0255\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9534, val_loss=4.9142, arbitrages_proportion=0.3166\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 256, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/50, Train Loss: 21.0902, Val Loss: 19.8280\n",
      "Epoch 2/50, Train Loss: 15.9752, Val Loss: 10.5303\n",
      "Epoch 3/50, Train Loss: 8.9989, Val Loss: 8.7228\n",
      "Epoch 4/50, Train Loss: 8.4653, Val Loss: 8.2066\n",
      "Epoch 5/50, Train Loss: 8.1252, Val Loss: 8.0962\n",
      "Epoch 6/50, Train Loss: 7.9118, Val Loss: 7.6171\n",
      "Epoch 7/50, Train Loss: 7.3938, Val Loss: 7.1278\n",
      "Epoch 8/50, Train Loss: 6.8626, Val Loss: 6.5996\n",
      "Epoch 9/50, Train Loss: 6.4077, Val Loss: 6.1785\n",
      "Epoch 10/50, Train Loss: 5.9850, Val Loss: 5.8506\n",
      "Epoch 11/50, Train Loss: 5.5742, Val Loss: 5.4203\n",
      "Epoch 12/50, Train Loss: 5.3580, Val Loss: 5.3410\n",
      "Epoch 13/50, Train Loss: 5.2139, Val Loss: 5.2396\n",
      "Epoch 14/50, Train Loss: 5.1851, Val Loss: 5.2879\n",
      "Epoch 15/50, Train Loss: 5.1905, Val Loss: 5.1750\n",
      "Epoch 16/50, Train Loss: 5.1641, Val Loss: 5.1921\n",
      "Epoch 17/50, Train Loss: 5.1380, Val Loss: 5.1522\n",
      "Epoch 18/50, Train Loss: 5.1071, Val Loss: 5.1542\n",
      "Epoch 19/50, Train Loss: 5.0925, Val Loss: 5.1504\n",
      "Epoch 20/50, Train Loss: 5.0689, Val Loss: 5.0958\n",
      "Epoch 21/50, Train Loss: 5.1200, Val Loss: 5.1652\n",
      "Epoch 22/50, Train Loss: 5.0657, Val Loss: 5.0498\n",
      "Epoch 23/50, Train Loss: 5.0498, Val Loss: 5.0640\n",
      "Epoch 24/50, Train Loss: 5.0436, Val Loss: 5.0913\n",
      "Epoch 25/50, Train Loss: 5.0330, Val Loss: 5.1159\n",
      "Epoch 26/50, Train Loss: 5.0449, Val Loss: 5.1373\n",
      "Epoch 27/50, Train Loss: 5.0540, Val Loss: 5.0344\n",
      "Epoch 28/50, Train Loss: 5.0169, Val Loss: 5.0670\n",
      "Epoch 29/50, Train Loss: 5.0100, Val Loss: 5.0517\n",
      "Epoch 30/50, Train Loss: 4.9919, Val Loss: 5.0480\n",
      "Epoch 31/50, Train Loss: 5.0059, Val Loss: 5.0053\n",
      "Epoch 32/50, Train Loss: 4.9876, Val Loss: 5.0327\n",
      "Epoch 33/50, Train Loss: 4.9955, Val Loss: 5.0561\n",
      "Epoch 34/50, Train Loss: 4.9874, Val Loss: 5.0721\n",
      "Epoch 35/50, Train Loss: 4.9992, Val Loss: 5.0122\n",
      "Epoch 36/50, Train Loss: 4.9893, Val Loss: 5.1020\n",
      "Epoch 37/50, Train Loss: 5.0034, Val Loss: 5.0961\n",
      "Epoch 38/50, Train Loss: 5.0179, Val Loss: 5.0098\n",
      "Epoch 39/50, Train Loss: 4.9958, Val Loss: 5.0385\n",
      "Epoch 40/50, Train Loss: 5.1803, Val Loss: 5.4329\n",
      "Epoch 41/50, Train Loss: 5.1687, Val Loss: 5.0450\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9874, val_loss=5.0053, arbitrages_proportion=0.2753\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 256, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 1000.0}\n",
      "Epoch 1/50, Train Loss: 20.8698, Val Loss: 18.9445\n",
      "Epoch 2/50, Train Loss: 13.2527, Val Loss: 9.1319\n",
      "Epoch 3/50, Train Loss: 8.8075, Val Loss: 8.4747\n",
      "Epoch 4/50, Train Loss: 8.6518, Val Loss: 8.6387\n",
      "Epoch 5/50, Train Loss: 8.4353, Val Loss: 8.5342\n",
      "Epoch 6/50, Train Loss: 8.3150, Val Loss: 8.2048\n",
      "Epoch 7/50, Train Loss: 8.0939, Val Loss: 7.9886\n",
      "Epoch 8/50, Train Loss: 7.6860, Val Loss: 7.4445\n",
      "Epoch 9/50, Train Loss: 7.0747, Val Loss: 6.7181\n",
      "Epoch 10/50, Train Loss: 6.4047, Val Loss: 6.1466\n",
      "Epoch 11/50, Train Loss: 5.8800, Val Loss: 5.7375\n",
      "Epoch 12/50, Train Loss: 5.4843, Val Loss: 5.3736\n",
      "Epoch 13/50, Train Loss: 5.2934, Val Loss: 5.2756\n",
      "Epoch 14/50, Train Loss: 5.2415, Val Loss: 5.2481\n",
      "Epoch 15/50, Train Loss: 5.1597, Val Loss: 5.2361\n",
      "Epoch 16/50, Train Loss: 5.1377, Val Loss: 5.1560\n",
      "Epoch 17/50, Train Loss: 5.1185, Val Loss: 5.1394\n",
      "Epoch 18/50, Train Loss: 5.1280, Val Loss: 5.1321\n",
      "Epoch 19/50, Train Loss: 5.0799, Val Loss: 5.1724\n",
      "Epoch 20/50, Train Loss: 5.0897, Val Loss: 5.1439\n",
      "Epoch 21/50, Train Loss: 5.0828, Val Loss: 5.2157\n",
      "Epoch 22/50, Train Loss: 5.0926, Val Loss: 5.1212\n",
      "Epoch 23/50, Train Loss: 5.0673, Val Loss: 5.1111\n",
      "Epoch 24/50, Train Loss: 5.0734, Val Loss: 5.0979\n",
      "Epoch 25/50, Train Loss: 5.0763, Val Loss: 5.0995\n",
      "Epoch 26/50, Train Loss: 5.0569, Val Loss: 5.0832\n",
      "Epoch 27/50, Train Loss: 5.0176, Val Loss: 5.0598\n",
      "Epoch 28/50, Train Loss: 5.0278, Val Loss: 5.0884\n",
      "Epoch 29/50, Train Loss: 5.0319, Val Loss: 5.0831\n",
      "Epoch 30/50, Train Loss: 5.0131, Val Loss: 5.0698\n",
      "Epoch 31/50, Train Loss: 5.0065, Val Loss: 5.0730\n",
      "Epoch 32/50, Train Loss: 5.0053, Val Loss: 5.0352\n",
      "Epoch 33/50, Train Loss: 5.0115, Val Loss: 5.0817\n",
      "Epoch 34/50, Train Loss: 5.0031, Val Loss: 5.0395\n",
      "Epoch 35/50, Train Loss: 4.9845, Val Loss: 5.0297\n",
      "Epoch 36/50, Train Loss: 4.9913, Val Loss: 5.0450\n",
      "Epoch 37/50, Train Loss: 4.9771, Val Loss: 5.0420\n",
      "Epoch 38/50, Train Loss: 4.9963, Val Loss: 5.0498\n",
      "Epoch 39/50, Train Loss: 4.9951, Val Loss: 5.0984\n",
      "Epoch 40/50, Train Loss: 4.9830, Val Loss: 5.0446\n",
      "Epoch 41/50, Train Loss: 4.9677, Val Loss: 4.9681\n",
      "Epoch 42/50, Train Loss: 4.9675, Val Loss: 5.0113\n",
      "Epoch 43/50, Train Loss: 4.9728, Val Loss: 4.9696\n",
      "Epoch 44/50, Train Loss: 4.9571, Val Loss: 4.9900\n",
      "Epoch 45/50, Train Loss: 5.1806, Val Loss: 5.3914\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.9571, val_loss=4.9681, arbitrages_proportion=0.4518\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 256, 'epochs': 50, 'patience': 10, 'lambda_butterfly': 10000.0}\n",
      "Epoch 1/50, Train Loss: 21.0711, Val Loss: 20.2211\n",
      "Epoch 2/50, Train Loss: 16.1431, Val Loss: 9.6012\n",
      "Epoch 3/50, Train Loss: 8.9038, Val Loss: 8.6451\n",
      "Epoch 4/50, Train Loss: 8.4415, Val Loss: 8.3372\n",
      "Epoch 5/50, Train Loss: 8.2692, Val Loss: 8.2396\n",
      "Epoch 6/50, Train Loss: 8.2404, Val Loss: 8.2558\n",
      "Epoch 7/50, Train Loss: 8.0786, Val Loss: 7.8940\n",
      "Epoch 8/50, Train Loss: 7.7888, Val Loss: 7.6124\n",
      "Epoch 9/50, Train Loss: 7.4262, Val Loss: 7.2109\n",
      "Epoch 10/50, Train Loss: 7.1248, Val Loss: 6.9713\n",
      "Epoch 11/50, Train Loss: 6.8342, Val Loss: 6.7407\n",
      "Epoch 12/50, Train Loss: 6.6271, Val Loss: 6.5665\n",
      "Epoch 13/50, Train Loss: 6.4798, Val Loss: 6.4301\n",
      "Epoch 14/50, Train Loss: 6.3414, Val Loss: 6.1985\n",
      "Epoch 15/50, Train Loss: 6.1726, Val Loss: 6.0963\n",
      "Epoch 16/50, Train Loss: 6.0279, Val Loss: 6.0370\n",
      "Epoch 17/50, Train Loss: 5.8818, Val Loss: 5.7791\n",
      "Epoch 18/50, Train Loss: 5.6827, Val Loss: 5.5712\n",
      "Epoch 19/50, Train Loss: 5.4294, Val Loss: 5.2588\n",
      "Epoch 20/50, Train Loss: 5.2417, Val Loss: 5.1518\n",
      "Epoch 21/50, Train Loss: 5.1358, Val Loss: 5.1514\n",
      "Epoch 22/50, Train Loss: 5.0663, Val Loss: 5.1172\n",
      "Epoch 23/50, Train Loss: 5.0780, Val Loss: 5.0754\n",
      "Epoch 24/50, Train Loss: 5.0756, Val Loss: 5.0512\n",
      "Epoch 25/50, Train Loss: 5.0674, Val Loss: 5.0969\n",
      "Epoch 26/50, Train Loss: 5.0307, Val Loss: 5.0636\n",
      "Epoch 27/50, Train Loss: 5.0188, Val Loss: 4.9978\n",
      "Epoch 28/50, Train Loss: 5.0401, Val Loss: 5.1083\n",
      "Epoch 29/50, Train Loss: 5.0524, Val Loss: 4.9811\n",
      "Epoch 30/50, Train Loss: 5.0120, Val Loss: 5.0494\n",
      "Epoch 31/50, Train Loss: 4.9528, Val Loss: 5.0300\n",
      "Epoch 32/50, Train Loss: 4.9974, Val Loss: 4.9409\n",
      "Epoch 33/50, Train Loss: 4.9672, Val Loss: 5.0446\n",
      "Epoch 34/50, Train Loss: 4.9704, Val Loss: 4.9883\n",
      "Epoch 35/50, Train Loss: 4.9355, Val Loss: 5.0039\n",
      "Epoch 36/50, Train Loss: 4.9960, Val Loss: 4.9719\n",
      "Epoch 37/50, Train Loss: 4.9413, Val Loss: 4.9967\n",
      "Epoch 38/50, Train Loss: 4.9444, Val Loss: 4.9867\n",
      "Epoch 39/50, Train Loss: 4.9355, Val Loss: 4.9892\n",
      "Epoch 40/50, Train Loss: 4.9390, Val Loss: 4.9994\n",
      "Epoch 41/50, Train Loss: 4.9283, Val Loss: 4.9510\n",
      "Epoch 42/50, Train Loss: 4.9450, Val Loss: 4.9525\n",
      "Epoch 43/50, Train Loss: 4.9372, Val Loss: 4.9338\n",
      "Epoch 44/50, Train Loss: 4.9427, Val Loss: 4.9083\n",
      "Epoch 45/50, Train Loss: 4.9129, Val Loss: 4.8907\n",
      "Epoch 46/50, Train Loss: 4.9071, Val Loss: 4.9383\n",
      "Epoch 47/50, Train Loss: 4.9023, Val Loss: 4.9429\n",
      "Epoch 48/50, Train Loss: 4.9294, Val Loss: 4.9178\n",
      "Epoch 49/50, Train Loss: 4.8964, Val Loss: 4.9758\n",
      "Epoch 50/50, Train Loss: 4.8869, Val Loss: 5.0438\n",
      "Early stopping triggered.\n",
      " --> Result: train_loss=4.8869, val_loss=4.8907, arbitrages_proportion=0.6146\n",
      "Testing hyperparams: {'lr': 0.001, 'hidden_dim': 64, 'latent_dim': 32, 'batch_size': 256, 'epochs': 50, 'patience': 20, 'lambda_butterfly': 0.0}\n",
      "Epoch 1/50, Train Loss: 20.8716, Val Loss: 18.6650\n",
      "Epoch 2/50, Train Loss: 15.1437, Val Loss: 11.2892\n",
      "Epoch 3/50, Train Loss: 9.2976, Val Loss: 8.7962\n",
      "Epoch 4/50, Train Loss: 8.5270, Val Loss: 8.5832\n",
      "Epoch 5/50, Train Loss: 8.3071, Val Loss: 8.0995\n",
      "Epoch 6/50, Train Loss: 8.0873, Val Loss: 7.7669\n",
      "Epoch 7/50, Train Loss: 7.6892, Val Loss: 7.3763\n",
      "Epoch 8/50, Train Loss: 7.0718, Val Loss: 6.5751\n",
      "Epoch 9/50, Train Loss: 6.3112, Val Loss: 5.9033\n",
      "Epoch 10/50, Train Loss: 5.6682, Val Loss: 5.5403\n",
      "Epoch 11/50, Train Loss: 5.3933, Val Loss: 5.3848\n",
      "Epoch 12/50, Train Loss: 5.3422, Val Loss: 5.3255\n",
      "Epoch 13/50, Train Loss: 5.2222, Val Loss: 5.2976\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 65536 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m val_loader   \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset,   batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 57\u001b[0m train_loss, val_loss, arbitrages_proportion \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_vae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_torch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmean_torch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatent_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m run_result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: val_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marbitrages_proportion\u001b[39m\u001b[38;5;124m'\u001b[39m: arbitrages_proportion}\n\u001b[0;32m     73\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(run_result)\n",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36mtrain_and_evaluate_vae\u001b[1;34m(train_loader, val_loader, data, scaler, scale_torch, mean_torch, lr, hidden_dim, latent_dim, epochs, device, patience, lambda_butterfly)\u001b[0m\n\u001b[0;32m     34\u001b[0m recon, mu, logvar \u001b[38;5;241m=\u001b[39m model(x_batch)\n\u001b[0;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m total_loss_with_butterfly(recon, x_batch, mu, logvar, lambda_butterfly, scale_torch, mean_torch)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 65536 bytes."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
